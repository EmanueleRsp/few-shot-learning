{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Generalized Few-Shot Learning (GFSL)**\n",
        "\n",
        "This demo implements **Dynamic Few-Shot Learning without Forgetting** (DFSLwF) on the **CIFAR-100** dataset under the *generalized setting*, where both base and novel classes are present at test time. The notebook reproduces the two-stage training pipeline (supervised base training followed by episodic meta-training with a generator) and allows readers to evaluate base, novel, and harmonic accuracies in the **GFSL scenario**."
      ],
      "metadata": {
        "id": "tFoEk1MspnUb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "PgPkCPJxpl63"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Background**"
      ],
      "metadata": {
        "id": "CFGadMzopx5K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.1 Generalized Few-Shot Learning (GFSL)**\n",
        "\n",
        "In GFSL the evaluation setup changes: during testing, queries can belong to either  \n",
        "- **base classes** (those already seen during training), or  \n",
        "- **novel classes** (new ones introduced only at test time).  \n",
        "\n",
        "The learner must recognize queries across this **joint label space**, handling both familiar and unseen categories simultaneously. This setting is more realistic than classical FSL, where queries always belong to novel classes only.\n",
        "\n",
        "> **Example (GFSL 3-way 2-shot):** suppose the model was trained on 64 base classes. At test time, an episode may include queries from both these base classes and 3 novel ones, each with only 2 support examples. The learner must correctly classify across all base + novel classes.\n",
        "\n",
        "A challenge arises: the model tends to be **biased toward base classes**, since they are represented by many more examples during training.  \n",
        "Approaches like **Dynamic Few-Shot Learning without Forgetting (DFSLwF)** mitigate this issue by combining a base classifier (trained on abundant data) with a novel classifier (trained from few supports), and balancing their predictions at inference time.\n",
        "\n",
        "*(Evaluation tip: in GFSL, report separate accuracy on base and novel classes, plus the **harmonic mean (H-mean)** to capture overall balance.)*\n",
        "\n",
        "> **Practical scenario:** GFSL is valuable in real-world systems (e.g., an image recognition app) where the model must keep performing well on frequent categories like “dogs” or “cars” while also learning to recognize new, rare categories from just a handful of examples.\n"
      ],
      "metadata": {
        "id": "tiLuW00wp55a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.2 Dynamic Few-Shot Learning without Forgetting**\n",
        "\n",
        "Let $F_\\theta(\\cdot)$ be a ConvNet feature extractor.  \n",
        "A classifier is built using a set of **weight vectors** $W = \\{ w_k \\}$, one per class, and computes scores via a **cosine similarity**:\n",
        "\n",
        "$$\n",
        "s_k(x) \\;=\\; \\tau \\cdot \\cos\\!\\big( F_\\theta(x), w_k \\big),\n",
        "$$\n",
        "\n",
        "where both features and weights are $\\ell_2$-normalized, and $\\tau$ is a learnable scale.  \n",
        "This design ensures that base and novel categories are treated in a **unified space**.\n",
        "\n",
        "To incorporate novel classes at test time, DFSLwF introduces a **few-shot weight generator** $G(\\cdot)$:\n",
        "- Input: a small support set of feature vectors $Z' = \\{z'_i\\}$ from a novel class, plus the set of base weights $W_\\text{base}$,  \n",
        "- Output: a novel classification weight $w'$ for that class.\n",
        "\n",
        "The generator combines two mechanisms:\n",
        "1. **Feature averaging:**  \n",
        "   $w'_\\text{avg} = \\tfrac{1}{N'} \\sum_i z'_i$, scaled by learnable parameters.  \n",
        "2. **Attention over base weights:**  \n",
        "   Composes $w'_\\text{att}$ as a similarity-weighted sum of $W_\\text{base}$, exploiting prior knowledge about the visual world.  \n",
        "\n",
        "The final novel weight is a linear combination:  \n",
        "$$\n",
        "w' = \\phi_\\text{avg} \\odot w'_\\text{avg} \\;+\\; \\phi_\\text{att} \\odot w'_\\text{att}.\n",
        "$$\n",
        "\n",
        "At inference, the classifier uses  \n",
        "$$\n",
        "W^* = W_\\text{base} \\cup W_\\text{novel},\n",
        "$$  \n",
        "thus predicting across **both base and novel classes** without retraining.\n",
        "\n",
        "**Training procedure (two stages):**\n",
        "1. **Stage 1:** Train feature extractor + base classifier on abundant base data.  \n",
        "2. **Stage 2:** Train the weight generator using “fake” novel tasks sampled from base categories (episodic style).  \n",
        "\n",
        "**Notes (implementation):**\n",
        "- Removing the final ReLU in $F_\\theta$ helps cosine similarity classification.  \n",
        "- The $\\ell_2$-normalization enforces compact clusters, improving generalization to novel classes.  \n",
        "- Evaluation reports accuracy on **Base**, **Novel**, and the **harmonic mean (H-mean)**.  \n",
        "\n",
        "> **Why it matters:** DFSLwF directly addresses the **GFSL setting**: real systems must keep high performance on frequent base categories (e.g., “dogs”, “cars”) while adapting dynamically to rare, unseen ones (e.g., “drone types”) from few examples, without retraining or forgetting.\n"
      ],
      "metadata": {
        "id": "ZJGmdC0np5IB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.3 CIFAR-100 dataset**\n",
        "We also use [CIFAR-100](https://www.cs.toronto.edu/~kriz/cifar.html), a widely adopted benchmark for image classification.  \n",
        "It consists of **100 classes** (e.g., animals, vehicles, household objects), each containing **600 color images** of size $32 \\times 32$.  \n",
        "For every class, there are **500 training images** and **100 test images**.\n",
        "\n",
        "CIFAR-100 is more challenging than Omniglot or MNIST-like datasets, since it involves **natural RGB images** with high intra-class variability and smaller image resolution.\n",
        "\n",
        "It's included in the `torchvision` package, making it straightforward to download and preprocess for few-shot or generalized few-shot experiments.\n"
      ],
      "metadata": {
        "id": "5CzeVUAMqNiN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "AXRqQlJspyNl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Practice**"
      ],
      "metadata": {
        "id": "UAwYkSFHoebf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "import math\n",
        "import random\n",
        "import types\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "import copy\n",
        "from functools import partial\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import ConcatDataset, DataLoader, Subset\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import CIFAR100\n",
        "from torchvision.models import resnet18, ResNet18_Weights\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "0c0fLQwq5t_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seed:"
      ],
      "metadata": {
        "id": "nlrbjXRRoo-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 42\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "train_rng = np.random.default_rng(SEED + 1)\n",
        "test_rng = np.random.default_rng(SEED + 2)"
      ],
      "metadata": {
        "id": "h0QZmdPV52XF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Settings:"
      ],
      "metadata": {
        "id": "_6NNqLzFoxUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Few-shot episode configuration\n",
        "N_WAY = 5           # Number of classes per task\n",
        "K_SHOT = 5          # Support examples per class\n",
        "Q_NOVEL = 15        # Query examples per novel class\n",
        "Q_BASE_TOTAL = 75   # Query examples of base classes\n",
        "\n",
        "# CIFAR-100 split sizes\n",
        "N_BASE = 64         # Number of classes retained as base\n",
        "N_VALNOVEL = 16     # Number of classes retained as novel (for validation)\n",
        "N_TESTNOVEL = 20    # Number of classes retained as novel (for test)\n",
        "\n",
        "# Network params\n",
        "TAU_INIT = 10.0             # Temperature init\n",
        "BACKBONE = \"cifar_resnet\"   # Feature extractor network\n",
        "assert BACKBONE in [\"resnet18\", \"conv\", \"cifar_resnet\"], f\"Unknown backbone '{BACKBONE}'. Use 'conv', 'resnet18' or 'cifar_resnet'.\"\n",
        "\n",
        "# Stage 1\n",
        "STAGE1_EPOCHS = 120\n",
        "STAGE1_LR = 3e-3\n",
        "STAGE1_BS = 512\n",
        "STAGE1_WEIGHT_DECAY = 5e-4\n",
        "S1_VAL_FRAC   = 0.10   # Fraction of classes retained for validation\n",
        "S1_VAL_EVERY  = 2      # Validate every N epochs\n",
        "S1_PATIENCE   = 5      # Early-stopping after N validations without improvements\n",
        "S1_LOG_EVERY  = 50     # Log loss every N batches\n",
        "\n",
        "# Stage 2\n",
        "STAGE2_TASKS = 20_000         # Number of training episodes\n",
        "STAGE2_LR = 5e-4\n",
        "# STAGE2_GRAD_CLIP = 1.0\n",
        "S2_VAL_TASKS      = 1_000     # Number of validation episodes\n",
        "STAGE2_VAL_EVERY = 500        # Validate every N episodes\n",
        "S2_PATIENCE       = 5         # Early-stopping after N validations without improvements\n",
        "S2_SELECT_METRIC  = \"hmean\"   # validation metric\n",
        "assert S2_SELECT_METRIC in [\"hmean\", \"base\", \"novel\"], f\"Unknown metric '{S2_SELECT_METRIC}'. Use 'hmean', 'base' or 'novel'.\"\n",
        "\n",
        "# Test\n",
        "TEST_TASKS = 1_000    # Number of test episodes"
      ],
      "metadata": {
        "id": "MvrrIQjKoxLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utility functions:"
      ],
      "metadata": {
        "id": "NOTdCwcdpGaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_cifar100_classes(seed: int, n_base=64, n_val=16, n_test=20):\n",
        "    \"\"\"Split CIFAR-100 class IDs into base/val-novel/test-novel sets.\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "    classes = np.arange(100); rng.shuffle(classes)\n",
        "    return classes[:n_base].tolist(), classes[n_base:n_base+n_val].tolist(), classes[n_base+n_val:n_base+n_val+n_test].tolist()\n",
        "\n",
        "\n",
        "def subset_by_classes(ds, keep):\n",
        "    \"\"\"Return a Subset containing only samples whose label is in `keep`.\n",
        "\n",
        "    Uses vectorized filtering over `ds.targets` to select the indices that\n",
        "    belong to the provided set of class IDs.\n",
        "    \"\"\"\n",
        "    t = np.array(ds.targets)\n",
        "    idx = np.nonzero(np.isin(t, keep))[0]\n",
        "    return Subset(ds, idx)\n",
        "\n",
        "\n",
        "def class_to_local_indices(subset):\n",
        "    \"\"\"Build a mapping class_id -> list of *local* indices within `subset`.\n",
        "\n",
        "    Iterates over the subset indices and groups them by their original class\n",
        "    ID (read from `subset.dataset.targets`). Useful for fast episodic sampling\n",
        "    (e.g., drawing K support and Q query images per class).\n",
        "\n",
        "    Example:\n",
        "    ```\n",
        "    {\n",
        "        0: [0, 5, 9, ...],\n",
        "        1: [1, 7, ...],\n",
        "        ...\n",
        "    }\n",
        "    ```\n",
        "    \"\"\"\n",
        "    t = np.array(subset.dataset.targets)\n",
        "    out = {}\n",
        "    for j, i in enumerate(subset.indices):\n",
        "        y = int(t[i])\n",
        "        (out.setdefault(y, [])).append(j)\n",
        "    return out\n",
        "\n",
        "\n",
        "def stratified_split_subset(subset: Subset, val_frac: float, seed: int):\n",
        "    \"\"\"\n",
        "    Split a Subset in (train_part, val_part),\n",
        "    maintaining proportions between each original CIFAR class.\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "    ds_targets = np.array(subset.dataset.targets)\n",
        "    # map: class_id -> local indexes list in Subset\n",
        "    cls2locals = {}\n",
        "    for j, i in enumerate(subset.indices):\n",
        "        y = int(ds_targets[i])\n",
        "        (cls2locals.setdefault(y, [])).append(j)\n",
        "\n",
        "    train_locals, val_locals = [], []\n",
        "    for y, locals_ in cls2locals.items():\n",
        "        locals_ = np.array(locals_, dtype=int)\n",
        "        n_val = max(1, int(round(len(locals_) * val_frac)))\n",
        "        rng.shuffle(locals_)\n",
        "        val_locals.append(locals_[:n_val])\n",
        "        train_locals.append(locals_[n_val:])\n",
        "\n",
        "    train_locals = np.concatenate(train_locals).tolist()\n",
        "    val_locals   = np.concatenate(val_locals).tolist()\n",
        "\n",
        "    train_indices = [subset.indices[i] for i in train_locals]\n",
        "    val_indices   = [subset.indices[i] for i in val_locals]\n",
        "\n",
        "    return Subset(subset.dataset, train_indices), Subset(subset.dataset, val_indices)"
      ],
      "metadata": {
        "id": "bRJokFSoC3Wo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def l2_normalize(x: torch.Tensor, dim: int = 1, eps: float = 1e-6) -> torch.Tensor:\n",
        "  \"\"\"L2-normalize a tensor along a given dimension.\n",
        "  \"\"\"\n",
        "  return x / (x.norm(p=2, dim=dim, keepdim=True).clamp_min(eps))"
      ],
      "metadata": {
        "id": "g6FYkq_npGJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_bn_eval(m: nn.Module):\n",
        "  \"\"\"Put BatchNorm2d layers in eval mode and freeze their parameters,\n",
        "  so it uses stored running statistics and stops updating them,\n",
        "  and it disables gradient updates for its affine parameters (gamma/beta).\n",
        "  \"\"\"\n",
        "  if isinstance(m, nn.BatchNorm2d):\n",
        "      m.eval()\n",
        "      for p in m.parameters():\n",
        "          p.requires_grad = False"
      ],
      "metadata": {
        "id": "nh0Z36WvpJ1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sliding_avg(xs: List[float], k: int = 20) -> float:\n",
        "  if not xs:\n",
        "      return 0.0\n",
        "  if len(xs) < k:\n",
        "      return float(sum(xs) / len(xs))\n",
        "  return float(sum(xs[-k:]) / k)"
      ],
      "metadata": {
        "id": "aXkLYzad5_UO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_ci95(xs: np.ndarray) -> Tuple[float, float]:\n",
        "    xs = np.asarray(xs, dtype=float)\n",
        "    n  = xs.size\n",
        "    if n < 2:\n",
        "        return float(xs.mean()), 0.0\n",
        "    std = xs.std(ddof=1)\n",
        "    stderr = std / np.sqrt(n)\n",
        "    z = 1.96\n",
        "    return float(xs.mean()), float(z * stderr)\n",
        "\n",
        "\n",
        "def gfsl_stats(\n",
        "    acc_per_ep_base: List[float],\n",
        "    acc_per_ep_novel: List[float],\n",
        ") -> Dict[str, Dict[str, float]]:\n",
        "\n",
        "    if len(acc_per_ep_base) != len(acc_per_ep_novel):\n",
        "        raise ValueError(\"base and novel must be of the same length\")\n",
        "    T = len(acc_per_ep_base)\n",
        "\n",
        "    base = np.asarray(acc_per_ep_base, dtype=float)\n",
        "    novel = np.asarray(acc_per_ep_novel, dtype=float)\n",
        "\n",
        "    denom = base + novel\n",
        "    h_per_ep = np.where(denom > 0, 2.0 * base * novel / denom, 0.0)\n",
        "\n",
        "    base_mean, base_ci  = mean_ci95(base)\n",
        "    novel_mean, novel_ci = mean_ci95(novel)\n",
        "    h_mean, h_ci = mean_ci95(h_per_ep)\n",
        "\n",
        "    return {\n",
        "        \"base\":  {\"mean\": base_mean,  \"conf\": base_ci},\n",
        "        \"novel\": {\"mean\": novel_mean, \"conf\": novel_ci},\n",
        "        \"hmean\": {\"mean\": h_mean,     \"conf\": h_ci},\n",
        "    }\n",
        "\n",
        "\n",
        "def print_stats(T: int, stats: Dict[str, Dict[str, float]], model: str = \"\"):\n",
        "  print(f\"[test] - {model} (95% CI on {T} tasks)\")\n",
        "  print(f\" - [Base]   acc={100*stats['base']['mean']:.2f}% ± {100*stats['base']['conf']:.2f}%\")\n",
        "  print(f\" - [Novel]  acc={100*stats['novel']['mean']:.2f}% ± {100*stats['novel']['conf']:.2f}%\")\n",
        "  print(f\" - [H-mean] acc={100*stats['hmean']['mean']:.2f}% ± {100*stats['hmean']['conf']:.2f}%\")"
      ],
      "metadata": {
        "id": "RWvm92IonBf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.1 Environment**"
      ],
      "metadata": {
        "id": "aRvCumaepV4w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.1.1 CIFAR100 dataset**\n",
        "\n",
        "We define **data transformations** separately for training and evaluation.  \n",
        "On **train**, we apply light stochastic augmentation suited to 32x32 images (random crop with padding and horizontal flip) to improve invariances without distorting small objects.  \n",
        "On **eval**, we keep a deterministic pipeline to ensure consistent measurement.\n",
        "\n",
        "> Augmentations reduce overfitting of the feature extractor in Stage-1. A stable eval pipeline is instead crucial when computing Base / Novel / H-mean in GFSL"
      ],
      "metadata": {
        "id": "NMpMP2wfpYWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if BACKBONE == \"conv\" or BACKBONE == \"cifar_resnet\":\n",
        "    IMAGE_SIZE = 32\n",
        "\n",
        "    train_tf = transforms.Compose([\n",
        "        transforms.RandomCrop(IMAGE_SIZE, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "    eval_tf = transforms.Compose([transforms.ToTensor(),])\n",
        "\n",
        "else:\n",
        "    IMNET_MEAN = [0.485, 0.456, 0.406]\n",
        "    IMNET_STD  = [0.229, 0.224, 0.225]\n",
        "    IM_RESIZE = 128\n",
        "    IM_CROP   = 112\n",
        "\n",
        "    train_tf = transforms.Compose([\n",
        "        transforms.Resize(IM_RESIZE),\n",
        "        transforms.RandomCrop(IM_CROP),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(IMNET_MEAN, IMNET_STD),\n",
        "    ])\n",
        "\n",
        "    eval_tf = transforms.Compose([\n",
        "        transforms.Resize(IM_RESIZE),\n",
        "        transforms.CenterCrop(IM_CROP),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(IMNET_MEAN, IMNET_STD),\n",
        "    ])"
      ],
      "metadata": {
        "id": "HwaG9qKI57p6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We instantiate the **GFSL** splits on CIFAR-100 as follows:\n",
        "- `ds_train` / `ds_test` load the standard CIFAR-100 **training** and **test** partitions with their respective transforms (`train_tf` randomized, `eval_tf` deterministic).\n",
        "- `split_cifar100_classes(SEED)` yields a **disjoint class split** into:\n",
        "  - **Base** classes (used for Stage-1 supervised training and as the base pool in GFSL episodes),\n",
        "  - **Val-Novel** classes (for model selection with GFSL episodes),\n",
        "  - **Test-Novel** classes (for final GFSL evaluation).\n",
        "\n",
        "All data live under `./data` and are fetched on demand with `download=True`.\n",
        "\n",
        "> *Note.* In GFSL, splits must be done **by class**, not by image, to avoid leakage. Label spaces are remapped locally for base vs. novel episodes so that evaluation is over the **joint label space** (Base ∪ Novel) while keeping indices compact inside each block.\n"
      ],
      "metadata": {
        "id": "AMHvYiUF1fKY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds_train = CIFAR100(root=\"./data\", train=True,  transform=train_tf, download=True)\n",
        "ds_test  = CIFAR100(root=\"./data\", train=False, transform=eval_tf,  download=True)\n",
        "\n",
        "base, valn, testn = split_cifar100_classes(SEED)"
      ],
      "metadata": {
        "id": "QcTRsE73tYyU",
        "outputId": "d3cf4efe-875b-4996-95e1-238708300f7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169M/169M [00:03<00:00, 42.7MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now **separate** the base training set into a train and validation split.  \n",
        "This allows us to monitor Stage-1 supervised training on base classes without leaking novel information.  \n",
        "We also prepare a distinct set of **val-novel classes** (from the train partition) to be used later for episodic validation in GFSL."
      ],
      "metadata": {
        "id": "k-d5W2u22YT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stage 1: Train + validation (base) / Stage 2: Train (base)\n",
        "train_base_full = subset_by_classes(ds_train, base)\n",
        "train_base_tr, train_base_val = stratified_split_subset(train_base_full, S1_VAL_FRAC, SEED)\n",
        "\n",
        "# Stage 2: Validation (Novel) - from train\n",
        "train_valnovel = subset_by_classes(ds_train, valn)\n",
        "\n",
        "# Test (base + novel)\n",
        "test_base  = subset_by_classes(ds_test,  base)\n",
        "test_novel = subset_by_classes(ds_test,  testn)"
      ],
      "metadata": {
        "id": "Zxqc9nIwBUTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we create the **label maps** for each split:  \n",
        "- local indices for base-train and base-val (Stage-1),  \n",
        "- mappings for val-novel classes (Stage-2 validation),  \n",
        "- and indices for base and novel classes in the final test set.  \n",
        "\n",
        "These compact class-to-indices maps are required for episodic samplers and joint logits during GFSL evaluation."
      ],
      "metadata": {
        "id": "FjpOdCMS2kPv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stage-1 (supervised)\n",
        "cti_train_base_tr  = class_to_local_indices(train_base_tr)\n",
        "cti_train_base_val = class_to_local_indices(train_base_val)\n",
        "\n",
        "# Stage-2 Validation (episodic GFSL) - from train\n",
        "cti_val_base   = cti_train_base_val\n",
        "cti_val_novel  = class_to_local_indices(train_valnovel)\n",
        "\n",
        "# For test (episodic GFSL)\n",
        "cti_test_base  = class_to_local_indices(test_base)\n",
        "cti_test_novel = class_to_local_indices(test_novel)"
      ],
      "metadata": {
        "id": "TTGwCpCPBRPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.1.1.1 DataLoader: Stage 1**"
      ],
      "metadata": {
        "id": "m0uFfFH8tbjP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stage-1 trains the backbone + base classifier with sufficient base data, so we construct a classic **supervised loader** over **base classes only**:"
      ],
      "metadata": {
        "id": "ahogH9-e4GFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Stage1TrainDS(torch.utils.data.Dataset):\n",
        "    def __init__(self, subset: Subset, orig_targets: List[int], order_map: Dict[int, int]):\n",
        "        self.subset = subset\n",
        "        self.local_labels = [order_map[int(orig_targets[i])] for i in subset.indices]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.subset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x, _ = self.subset[idx]\n",
        "        return x, self.local_labels[idx]\n",
        "\n",
        "\n",
        "base_order = sorted(base)\n",
        "order_map  = {cid: i for i, cid in enumerate(base_order)}\n",
        "\n",
        "stage1_train_ds = Stage1TrainDS(train_base_tr,  ds_train.targets, order_map)  # train\n",
        "stage1_val_ds   = Stage1TrainDS(train_base_val, ds_train.targets, order_map)  # val (disjoined)"
      ],
      "metadata": {
        "id": "bIui-IjnGOfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We instantiate them:"
      ],
      "metadata": {
        "id": "SKMzCEqO40Cm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader_s1 = DataLoader(\n",
        "    stage1_train_ds, batch_size=STAGE1_BS, shuffle=True,\n",
        "    num_workers=2, pin_memory=True\n",
        ")\n",
        "\n",
        "val_loader_s1   = DataLoader(\n",
        "    stage1_val_ds, batch_size=STAGE1_BS*2, shuffle=False,\n",
        "    num_workers=2, pin_memory=True\n",
        ")"
      ],
      "metadata": {
        "id": "7UGqx2oStiVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.1.1.2 DataLoader: Stage 2 Train**"
      ],
      "metadata": {
        "id": "yGENDGSFtivp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the stage-2 training (but also for the validation and test phases), we need to evaluate the on **few-shot tasks**.\n",
        "A standard PyTorch `DataLoader` builds mini-batches of images without\n",
        "considering whether they belong to a support or query set.\n",
        "\n",
        "However, in a GFSL setting, the dataloader works differently from the classical FSL one, since we need episodes that combine **few-shot novel supports** with **base queries**.\n",
        "\n",
        "To achieve this, we use a **custom task sampler** and a **custom collate function**."
      ],
      "metadata": {
        "id": "-gmGPd3E8ZTM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Collate function**\n",
        "\n",
        "It splits the batch into:\n",
        "  1. `support_novel` images (size `n_way * k_shot`)  \n",
        "  2. `query_images` = `[novel queries | base queries]`  \n",
        "  3. `true_novel_ids` (original CIFAR-100 ids for the sampled novel classes)  \n",
        "  4. `base_query_labels_cifar` (original CIFAR-100 ids for base queries)"
      ],
      "metadata": {
        "id": "MLgR1QJy-VQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stage2_collate(batch, n_way: int, k_shot: int, q_novel: int, q_base_total: int):\n",
        "    imgs, labs = list(zip(*batch))\n",
        "    images = torch.stack(imgs)\n",
        "    labels = torch.tensor([int(y) for y in labs])\n",
        "\n",
        "    per_novel = k_shot + q_novel\n",
        "    total_novel = n_way * per_novel\n",
        "\n",
        "    novel_block = images[:total_novel].view(n_way, per_novel, *images.shape[1:])\n",
        "    novel_labels_block = labels[:total_novel].view(n_way, per_novel)\n",
        "\n",
        "    support_novel = novel_block[:, :k_shot].reshape(-1, *images.shape[1:])\n",
        "    query_novel = novel_block[:, k_shot:].reshape(-1, *images.shape[1:])\n",
        "    query_base = images[total_novel:]\n",
        "\n",
        "    query_images = torch.cat([query_novel, query_base], dim=0)\n",
        "\n",
        "    true_novel_ids = [int(novel_labels_block[i, 0].item()) for i in range(n_way)]\n",
        "    base_query_labels_cifar = labels[total_novel:]  # original CIFAR ids\n",
        "    return support_novel, query_images, true_novel_ids, base_query_labels_cifar\n",
        "\n",
        "\n",
        "stage2_collate_fn = partial(\n",
        "    stage2_collate,\n",
        "    n_way=N_WAY, k_shot=K_SHOT, q_novel=Q_NOVEL, q_base_total=Q_BASE_TOTAL,\n",
        ")"
      ],
      "metadata": {
        "id": "VKDRe5296RFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Batch sampler**\n",
        "\n",
        "Samples `n_way` pseudo-novel classes from the base pool, drawing `k_shot + q_novel` examples per class. It then adds `q_base_total` queries from the remaining base classes.\n"
      ],
      "metadata": {
        "id": "RpUErJzs-Mjl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GFSLTrainEpisodicBatchSampler:\n",
        "    \"\"\"\n",
        "    Stage-2: N_WAY pseudo-novel (from BASE train) with K+Qn each + Qb base queries from BASE (any class).\n",
        "    Returns indices over the Subset(train_base).\n",
        "    \"\"\"\n",
        "    def __init__(self, class_to_indices: Dict[int, List[int]], n_tasks: int, rng: np.random.Generator,\n",
        "                 n_way: int = N_WAY, k_shot: int = K_SHOT, q_novel: int = Q_NOVEL, q_base_total: int = Q_BASE_TOTAL):\n",
        "        self.cti = class_to_indices\n",
        "        self.all_classes = list(class_to_indices.keys())\n",
        "        self.n_tasks = n_tasks\n",
        "        self.rng = rng\n",
        "        self.n_way = n_way\n",
        "        self.k_shot = k_shot\n",
        "        self.q_novel = q_novel\n",
        "        self.q_base_total = q_base_total\n",
        "\n",
        "    def __len__(self): return self.n_tasks\n",
        "\n",
        "    def __iter__(self):\n",
        "        for _ in range(self.n_tasks):\n",
        "\n",
        "            novel = self.rng.choice(self.all_classes, size=self.n_way, replace=False)\n",
        "\n",
        "            batch = []\n",
        "            for c in novel:\n",
        "                pool = self.cti[c]\n",
        "                need = self.k_shot + self.q_novel\n",
        "                if len(pool) < need:\n",
        "                    raise ValueError(f\"Class {c} has {len(pool)} < {need}\")\n",
        "                idx = self.rng.choice(pool, size=need, replace=False)\n",
        "                batch.append(idx)\n",
        "\n",
        "            novel_classes = set(novel.tolist())\n",
        "            base_pool_classes = [c for c in self.all_classes if c not in novel_classes]\n",
        "\n",
        "            used = set(np.concatenate(batch).tolist())\n",
        "\n",
        "            base_q = []\n",
        "            while len(base_q) < self.q_base_total:\n",
        "                c = int(self.rng.choice(base_pool_classes))\n",
        "                cand = int(self.rng.choice(self.cti[c]))\n",
        "                if cand not in used:\n",
        "                    base_q.append(cand)\n",
        "                    used.add(cand)\n",
        "\n",
        "            batch.append(np.array(base_q, dtype=int))\n",
        "            yield np.concatenate(batch)\n",
        "\n",
        "\n",
        "stage2_train_batch_sampler = GFSLTrainEpisodicBatchSampler(\n",
        "    cti_train_base_tr, n_tasks=STAGE2_TASKS, rng=train_rng,\n",
        "    n_way=N_WAY, k_shot=K_SHOT, q_novel=Q_NOVEL, q_base_total=Q_BASE_TOTAL\n",
        ")"
      ],
      "metadata": {
        "id": "jWdAwrxL6KH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now create our Dataset..."
      ],
      "metadata": {
        "id": "1h_2qLA3-bY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Stage2TrainDS(torch.utils.data.Dataset):\n",
        "    def __init__(self, subset: Subset):\n",
        "        self.subset = subset\n",
        "    def __len__(self):\n",
        "      return len(self.subset)\n",
        "    def __getitem__(self, idx):\n",
        "      return self.subset[idx]\n",
        "\n",
        "\n",
        "stage2_train_ds = Stage2TrainDS(train_base_tr)"
      ],
      "metadata": {
        "id": "zyl7POdxWpoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "... and build the DataLoader:"
      ],
      "metadata": {
        "id": "5ViakuhC-f_x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader_s2 = DataLoader(\n",
        "    stage2_train_ds, batch_sampler=stage2_train_batch_sampler,\n",
        "    collate_fn=stage2_collate_fn, num_workers=2, pin_memory=True\n",
        ")"
      ],
      "metadata": {
        "id": "0-sHBaXruHqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.1.1.3 DataLoader: Stage 2 Validation and Test**"
      ],
      "metadata": {
        "id": "mxGopBG5u_6A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For validation and test we need a **different sampler + collate** than in training, because episodes must follow the **GFSL evaluation protocol**:  \n",
        "- sample supports and queries only from the **novel split**,  \n",
        "- add a fixed number of queries from the **base split**,  \n",
        "- keep the layout `[novel block | base block]` so evaluation can compute accuracy on Base, Novel, and H-mean.  \n",
        "\n",
        "The custom sampler/collate ensure this structure and prevent any leakage between base and novel classes.\n"
      ],
      "metadata": {
        "id": "PPhKuotIACu1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_collate(batch, n_way: int, k_shot: int, q_novel: int, q_base_total: int):\n",
        "    \"\"\"Reconstruct support/query tensors for a GFSL test episode.\n",
        "\n",
        "    Input `batch` is a list of (image, label) from `test_concat` where the first\n",
        "    N_WAY*(K_SHOT+Q_NOVEL) items belong to the novel subset and the remaining Q_BASE_TOTAL\n",
        "    items belong to the base subset.\n",
        "    \"\"\"\n",
        "    imgs, labs = list(zip(*batch))\n",
        "    images = torch.stack(imgs)\n",
        "    labels = torch.tensor([int(y) for y in labs])\n",
        "\n",
        "    per_novel = k_shot + q_novel\n",
        "    total_novel = n_way * per_novel\n",
        "\n",
        "    # reshape novel block into (N, K+Qn, C, H, W) and (N, K+Qn) for labels\n",
        "    novel_block = images[:total_novel].view(n_way, per_novel, *images.shape[1:])\n",
        "    novel_labels_block = labels[:total_novel].view(n_way, per_novel)\n",
        "\n",
        "    # split into support (first K) and query (last Qn)\n",
        "    support_novel = novel_block[:, :k_shot].reshape(-1, *images.shape[1:])\n",
        "    query_novel   = novel_block[:, k_shot:].reshape(-1, *images.shape[1:])\n",
        "    query_base    = images[total_novel:]  # remaining Qb from base subset\n",
        "\n",
        "    # concatenate all queries [novel | base]\n",
        "    query_images = torch.cat([query_novel, query_base], dim=0)\n",
        "\n",
        "    # collect true novel CIFAR IDs (one per class) and per-query GT labels\n",
        "    true_novel_ids = [int(novel_labels_block[i, 0].item()) for i in range(n_way)]\n",
        "    gt_novel = novel_labels_block[:, k_shot:].reshape(-1)\n",
        "    gt_base  = labels[total_novel:]\n",
        "\n",
        "    return support_novel, query_images, true_novel_ids, gt_novel, gt_base\n",
        "\n",
        "\n",
        "eval_collate_fn = partial(\n",
        "    eval_collate,\n",
        "    n_way=N_WAY, k_shot=K_SHOT, q_novel=Q_NOVEL, q_base_total=Q_BASE_TOTAL,\n",
        ")"
      ],
      "metadata": {
        "id": "PmIR_s8bVaDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GFSLEvalEpisodeSampler:\n",
        "    \"\"\"Batch sampler for GFSL test episodes over a single ConcatDataset.\n",
        "\n",
        "    Each yielded batch is a 1D numpy array of indices into `test_concat`\n",
        "    laid out as:\n",
        "        [ N_WAY*(K_SHOT+Q_NOVEL) indices from novel part | Q_BASE_TOTAL indices from base part ]\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, base_cti: Dict[int, List[int]], novel_cti: Dict[int, List[int]], offset_base: int,\n",
        "                 n_tasks: int, rng: np.random.Generator,\n",
        "                 n_way: int = N_WAY, k_shot: int = K_SHOT, q_novel: int = Q_NOVEL, q_base_total: int = Q_BASE_TOTAL):\n",
        "        self.base_cti = base_cti\n",
        "        self.novel_cti = novel_cti\n",
        "        self.offset_base = offset_base\n",
        "\n",
        "        self.base_classes  = list(base_cti.keys())\n",
        "        self.novel_classes = list(novel_cti.keys())\n",
        "\n",
        "        self.n_tasks = n_tasks\n",
        "        self.rng = rng\n",
        "        self.n_way = n_way\n",
        "        self.k_shot = k_shot\n",
        "        self.q_novel = q_novel\n",
        "        self.q_base_total = q_base_total\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_tasks\n",
        "\n",
        "    def __iter__(self):\n",
        "        for _ in range(self.n_tasks):\n",
        "            # ---- NOVEL BLOCK (test_novel) ----\n",
        "            chosen_novel = self.rng.choice(self.novel_classes, size=self.n_way, replace=False)\n",
        "            per_novel = self.k_shot + self.q_novel\n",
        "\n",
        "            novel_chunks = []\n",
        "            for c in chosen_novel:\n",
        "                pool = self.novel_cti[c]  # local indices within test_novel\n",
        "                if len(pool) < per_novel:\n",
        "                    raise ValueError(f\"Novel class {c} has {len(pool)} < {per_novel}\")\n",
        "                idx = self.rng.choice(pool, size=per_novel, replace=False)\n",
        "                novel_chunks.append(idx)\n",
        "\n",
        "            novel_block = np.concatenate(novel_chunks).astype(int)  # still in \"novel\" namespace (no offset)\n",
        "\n",
        "            # ---- BASE BLOCK (test_base) — NO DUPLICATES, NO REPLACEMENT ----\n",
        "            # Build a single pool of local indices from all base classes\n",
        "            base_pool = np.concatenate([self.base_cti[c] for c in self.base_classes]) if self.base_classes else np.array([], dtype=int)\n",
        "            if len(base_pool) < self.q_base_total:\n",
        "                raise ValueError(f\"Not enough base candidates: have {len(base_pool)} < {self.q_base_total}\")\n",
        "\n",
        "            # Sample Qb distinct local indices, then shift by offset to address ConcatDataset second component\n",
        "            base_q_local = self.rng.choice(base_pool, size=self.q_base_total, replace=False)\n",
        "            base_block   = (base_q_local + self.offset_base).astype(int)\n",
        "\n",
        "            # ---- FINAL EPISODE ----\n",
        "            full_episode = np.concatenate([novel_block, base_block])\n",
        "            yield full_episode"
      ],
      "metadata": {
        "id": "V1IuPKZRVVzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validation (Stage-2) dataLoader:"
      ],
      "metadata": {
        "id": "t6ylrgepIMi3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampler_val_s2 = GFSLEvalEpisodeSampler(\n",
        "    cti_val_base, cti_val_novel, len(train_valnovel), n_tasks=S2_VAL_TASKS, rng=test_rng,\n",
        "    n_way=N_WAY, k_shot=K_SHOT, q_novel=Q_NOVEL, q_base_total=Q_BASE_TOTAL\n",
        ")"
      ],
      "metadata": {
        "id": "jWw60OHxI8gv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a single test dataset by concatenating the two subsets.\n",
        "# Order matters: novel part first, base part second.\n",
        "val_concat_s2 = ConcatDataset([train_valnovel, train_base_val])"
      ],
      "metadata": {
        "id": "iveYcIVtIeoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_loader_s2 = DataLoader(\n",
        "    val_concat_s2, batch_sampler=sampler_val_s2, collate_fn=eval_collate_fn,\n",
        "    num_workers=2, pin_memory=True\n",
        ")"
      ],
      "metadata": {
        "id": "2aeVNuJ3IORR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test DataLoader:"
      ],
      "metadata": {
        "id": "tCYjO5btILKW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampler_test = GFSLEvalEpisodeSampler(\n",
        "    cti_test_base, cti_test_novel, len(test_novel), n_tasks=TEST_TASKS, rng=test_rng,\n",
        "    n_way=N_WAY, k_shot=K_SHOT, q_novel=Q_NOVEL, q_base_total=Q_BASE_TOTAL\n",
        ")"
      ],
      "metadata": {
        "id": "_5qt7i5sIppg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a single test dataset by concatenating the two subsets.\n",
        "# Order matters: novel part first, base part second.\n",
        "test_concat = ConcatDataset([test_novel, test_base])"
      ],
      "metadata": {
        "id": "R0ZCnKHVfmo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader = DataLoader(\n",
        "    test_concat, batch_sampler=sampler_test, collate_fn=eval_collate_fn,\n",
        "    num_workers=2, pin_memory=True\n",
        ")"
      ],
      "metadata": {
        "id": "l5nkZ9_uvDbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.1.2 DFSLwF module**\n",
        "\n",
        "???"
      ],
      "metadata": {
        "id": "Q4Zt-0FOrjzs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.1.2.1 Feature extractor**\n",
        "\n",
        "???"
      ],
      "metadata": {
        "id": "8HUI14SjArDQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicBlockCIFAR(nn.Module):\n",
        "    expansion = 1\n",
        "    def __init__(self, in_ch, out_ch, stride=1, remove_last_relu=False):\n",
        "        super().__init__()\n",
        "        self.remove_last_relu = remove_last_relu\n",
        "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1   = nn.BatchNorm2d(out_ch)\n",
        "        self.relu  = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, stride=1, padding=1, bias=False)\n",
        "        self.bn2   = nn.BatchNorm2d(out_ch)\n",
        "        self.down  = None\n",
        "        if stride != 1 or in_ch != out_ch:\n",
        "            self.down = nn.Sequential(\n",
        "                nn.Conv2d(in_ch, out_ch, 1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_ch),\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        out = self.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        if self.down is not None:\n",
        "            identity = self.down(x)\n",
        "        out = out + identity\n",
        "        if not self.remove_last_relu:\n",
        "            out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class CIFARResNetSmall(nn.Module):\n",
        "    \"\"\"\n",
        "    ResNet 'cifar-like' per 32x32:\n",
        "      stem 3x3 s1 -> [64]x2 -> [128]x2 (s2) -> [256]x2 (s2) -> GAP -> 256-D\n",
        "    \"\"\"\n",
        "    def __init__(self, remove_last_relu: bool = True):\n",
        "        super().__init__()\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.layer1 = nn.Sequential(\n",
        "            BasicBlockCIFAR(64, 64, 1),\n",
        "            BasicBlockCIFAR(64, 64, 1),\n",
        "        )\n",
        "        self.layer2 = nn.Sequential(\n",
        "            BasicBlockCIFAR(64, 128, 2),   # downsample 32->16\n",
        "            BasicBlockCIFAR(128, 128, 1),\n",
        "        )\n",
        "        self.layer3 = nn.Sequential(\n",
        "            BasicBlockCIFAR(128, 256, 2),  # downsample 16->8\n",
        "            BasicBlockCIFAR(256, 256, 1, remove_last_relu=remove_last_relu),\n",
        "        )\n",
        "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
        "        self.out_dim = 256\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)\n",
        "        x = self.layer1(x); x = self.layer2(x); x = self.layer3(x)\n",
        "        x = self.gap(x).squeeze(-1).squeeze(-1)  # (B,256)\n",
        "        return x"
      ],
      "metadata": {
        "id": "hH7c_e8POaS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvBlock(nn.Module):\n",
        "    \"\"\"Conv3x3 -> BN -> ReLU -> (optional MaxPool2d).\"\"\"\n",
        "    def __init__(self, in_ch, out_ch, pool: bool):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn   = nn.BatchNorm2d(out_ch)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.pool = nn.MaxPool2d(2) if pool else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x); x = self.bn(x); x = self.relu(x); x = self.pool(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "1eh6yJqnOcFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureExtractor(nn.Module):\n",
        "    \"\"\"Feature extractor with selectable backbone: 'conv' (light), 'cifar_resnet' or 'resnet18'.\n",
        "\n",
        "    Args:\n",
        "        backbone: 'conv', 'resnet18' or cifar_resnet.\n",
        "        normalize_out: if True, L2-normalize the output features.\n",
        "        resnet_pretrained: if True (only for 'resnet18'), load ImageNet pretrained weights.\n",
        "        remove_last_relu: if True (only for 'resnet18'), remove the last post-add ReLU in the final BasicBlock\n",
        "                          (useful with cosine classifiers).\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        backbone: str = \"conv\",\n",
        "        normalize_out: bool = True,\n",
        "        resnet_pretrained: bool = True,\n",
        "        remove_last_relu: bool = True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.normalize_out = normalize_out\n",
        "\n",
        "        if backbone.lower() == \"conv\":\n",
        "            # Lightweight CIFAR-style CNN trained from scratch: 64-D output\n",
        "            self.fe = nn.Sequential(\n",
        "                ConvBlock(3,   64, pool=True),   # 32 -> 16\n",
        "                ConvBlock(64,  64, pool=True),   # 16 -> 8\n",
        "                ConvBlock(64,  64, pool=False),\n",
        "                ConvBlock(64,  64, pool=False),\n",
        "                nn.AdaptiveAvgPool2d(1),         # -> (B,64,1,1)\n",
        "            )\n",
        "            self._mode = \"conv\"\n",
        "            self.out_dim = 64\n",
        "\n",
        "        elif backbone.lower() == \"cifar_resnet\":\n",
        "            self.fe = CIFARResNetSmall(remove_last_relu=remove_last_relu)\n",
        "            self._mode = \"cifar_resnet\"; self.out_dim = 256\n",
        "\n",
        "        elif backbone.lower() == \"resnet18\":\n",
        "            weights = ResNet18_Weights.IMAGENET1K_V1 if resnet_pretrained else None\n",
        "            m = resnet18(weights=weights)\n",
        "            m.fc = nn.Identity()  # we want the 512-D penultimate features\n",
        "\n",
        "            if remove_last_relu:\n",
        "                # Patch only the final BasicBlock to skip the post-add ReLU\n",
        "                last_block = m.layer4[-1]\n",
        "\n",
        "                def forward_norelu(self_block, x):\n",
        "                    identity = x\n",
        "                    out = self_block.conv1(x); out = self_block.bn1(out); out = self_block.relu(out)\n",
        "                    out = self_block.conv2(out); out = self_block.bn2(out)\n",
        "                    if self_block.downsample is not None:\n",
        "                        identity = self_block.downsample(x)\n",
        "                    out = out + identity\n",
        "                    return out  # no final ReLU\n",
        "\n",
        "                last_block.forward = types.MethodType(forward_norelu, last_block)\n",
        "\n",
        "            self.fe = m\n",
        "            self._mode = \"resnet18\"\n",
        "            self.out_dim = 512\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown backbone '{backbone}'. Use 'conv', 'resnet18' or 'cifar_resnet'.\")\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if self._mode == \"conv\":\n",
        "            z = self.fe(x).squeeze(-1).squeeze(-1)    # (B, 64)\n",
        "        else:  # resnet18\n",
        "            z = self.fe(x)                            # (B, 512)\n",
        "        return l2_normalize(z, dim=1) if self.normalize_out else z"
      ],
      "metadata": {
        "id": "BmbYd7mBr2y3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.1.2.2 Cosine Classifier**\n",
        "\n",
        "???"
      ],
      "metadata": {
        "id": "7hDoYa5fA2zv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CosineClassifier(nn.Module):\n",
        "    \"\"\"Cosine classifier with learnable temperature τ (tau).\"\"\"\n",
        "    def __init__(self, in_dim: int, n_classes: int, init_scale: float = TAU_INIT):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.empty(n_classes, in_dim))\n",
        "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
        "        self.tau = nn.Parameter(torch.tensor(float(init_scale)))\n",
        "\n",
        "    def forward(self, feats: torch.Tensor) -> torch.Tensor:\n",
        "        W = l2_normalize(self.weight, dim=1)\n",
        "        feats = l2_normalize(feats, dim=1)\n",
        "        logits = feats @ W.t()\n",
        "        return self.tau * logits"
      ],
      "metadata": {
        "id": "6DmrwTPYr3Xd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.1.2.3 Few-Shot Weight Generator**\n",
        "\n",
        "???"
      ],
      "metadata": {
        "id": "Z82kcDtbA8Aw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FewShotWeightGenerator(nn.Module):\n",
        "    \"\"\"\n",
        "    [DFSLwF] Few-shot classification weight generator = Avg + Attention:\n",
        "      - w_avg  = mean(z_i)                     → φ_avg ⊙ w_avg\n",
        "      - w_att  = avg_i softmax(γ cos(φ_q z_i, k_b)) · w_b (over base classes b) → φ_att ⊙ w_att\n",
        "      - w'     = φ_avg ⊙ w_avg + φ_att ⊙ w_att, then L2-normalize\n",
        "    Includes:\n",
        "      - learnable keys k_b (one per base class), size (C_base, D)\n",
        "      - learnable φ_q (Linear D→D, no bias), φ_avg, φ_att (vectors), γ (scalar)\n",
        "      - optional dropout on features during training (Stage-2)\n",
        "    \"\"\"\n",
        "    def __init__(self, dim: int, num_base: int, p_dropout: float = 0.5):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.num_base = num_base\n",
        "        self.phi_q = nn.Linear(dim, dim, bias=False)\n",
        "        nn.init.kaiming_uniform_(self.phi_q.weight, a=math.sqrt(5))\n",
        "        self.keys = nn.Parameter(l2_normalize(torch.randn(num_base, dim), dim=1))\n",
        "        self.phi_avg = nn.Parameter(torch.ones(dim))\n",
        "        self.phi_att = nn.Parameter(torch.ones(dim))\n",
        "        self.gamma = nn.Parameter(torch.tensor(10.0))  # attention temperature (like τ)\n",
        "        self.dropout = nn.Dropout(p=p_dropout)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        support_feats: torch.Tensor,          # (N*K, D), L2-normalized\n",
        "        base_weights: torch.Tensor,           # (C_base, D), not necessarily normalized\n",
        "        shots_per_class: int,\n",
        "        exclude_mask: Optional[torch.Tensor] = None  # (C_base,) bool; True = keep; False = exclude\n",
        "    ) -> torch.Tensor:\n",
        "        D = support_feats.size(1)\n",
        "        N = support_feats.size(0) // shots_per_class\n",
        "        z = support_feats.view(N, shots_per_class, D)\n",
        "        if self.training:\n",
        "            z = self.dropout(z)\n",
        "        # w_avg\n",
        "        w_avg = l2_normalize(z.mean(dim=1), dim=1)  # (N, D)\n",
        "        # attention over base weights\n",
        "        Wb = l2_normalize(base_weights, dim=1)      # (C_base, D)\n",
        "        Kb = l2_normalize(self.keys, dim=1)         # (C_base, D)\n",
        "        # queries\n",
        "        q = self.phi_q(z)                           # (N, K, D)\n",
        "        q = l2_normalize(q, dim=2)                 # normalize across D\n",
        "        # cosine(q, Kb) => (N, K, C_base)\n",
        "        att_logits = torch.einsum(\"nkd,bd->nkb\", q, Kb) * self.gamma\n",
        "        if exclude_mask is not None:\n",
        "            # set -inf on excluded classes before softmax\n",
        "            mask = exclude_mask.view(1, 1, -1)  # broadcast\n",
        "            att_logits = att_logits.masked_fill(~mask, float(\"-inf\"))\n",
        "        att = torch.softmax(att_logits, dim=2)      # (N, K, C_base)\n",
        "        # weighted sum of base weights -> (N, K, D), then average over K (shots)\n",
        "        w_att = torch.einsum(\"nkb,bd->nkd\", att, Wb).mean(dim=1)  # (N, D)\n",
        "        # combine\n",
        "        w = self.phi_avg * w_avg + self.phi_att * w_att            # Hadamard\n",
        "        w = l2_normalize(w, dim=1)  # (N, D)\n",
        "        return w"
      ],
      "metadata": {
        "id": "1SVW654R6Fmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.1.2.4 Final Network**\n",
        "\n",
        "???"
      ],
      "metadata": {
        "id": "gbYrjeFfBE86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DFSLwF(nn.Module):\n",
        "    def __init__(self, fe: FeatureExtractor, clf_base: CosineClassifier, gen: FewShotWeightGenerator):\n",
        "        super().__init__()\n",
        "        self.fe = fe\n",
        "        self.clf_base = clf_base\n",
        "        self.gen = gen\n",
        "\n",
        "    def forward_logits(self, x: torch.Tensor, novel_weights: torch.Tensor | None = None) -> torch.Tensor:\n",
        "        feats = self.fe(x)\n",
        "        Wb = l2_normalize(self.clf_base.weight, dim=1)\n",
        "        logits = self.clf_base.tau * (feats @ Wb.t())\n",
        "        if novel_weights is not None and novel_weights.numel() > 0:\n",
        "            Wn = l2_normalize(novel_weights, dim=1)\n",
        "            logits_n = self.clf_base.tau * (feats @ Wn.t())\n",
        "            logits = torch.cat([logits, logits_n], dim=1)\n",
        "        return logits\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def build_novel_weights(self, support_imgs: torch.Tensor, k_shot: int) -> torch.Tensor:\n",
        "        supp = self.fe(support_imgs)  # (N*K, D), already normalized\n",
        "        Wb = self.clf_base.weight\n",
        "        return self.gen(supp, Wb, k_shot, exclude_mask=None)"
      ],
      "metadata": {
        "id": "Sf1nyTmgsJmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can build our network following the initial settings:"
      ],
      "metadata": {
        "id": "mUnlmgXkBQKE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if BACKBONE == \"conv\":\n",
        "    fe = FeatureExtractor(backbone=BACKBONE, normalize_out=True)\n",
        "elif BACKBONE == \"cifar_resnet\":\n",
        "    fe = FeatureExtractor(backbone=BACKBONE, normalize_out=True, remove_last_relu=True)\n",
        "elif BACKBONE == \"resnet18\":\n",
        "    fe = FeatureExtractor(backbone=BACKBONE, normalize_out=True, resnet_pretrained=True, remove_last_relu=True)\n",
        "else:\n",
        "    raise ValueError(f\"Unknown backbone '{BACKBONE}'. Use 'conv', 'resnet18' or 'cifar_resnet'.\")\n",
        "\n",
        "clf = CosineClassifier(in_dim=fe.out_dim, n_classes=len(base_order))\n",
        "gen = FewShotWeightGenerator(dim=fe.out_dim, num_base=len(base_order), p_dropout=0.5)\n",
        "\n",
        "model = DFSLwF(fe=fe, clf_base=clf, gen=gen).to(device)"
      ],
      "metadata": {
        "id": "zXx49_x5vS-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_model = copy.deepcopy(model)"
      ],
      "metadata": {
        "id": "V6-K32td3uTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.2 Training**\n",
        "\n",
        "???"
      ],
      "metadata": {
        "id": "pwqrdqc9DitQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.2.1 Stage 1: supervised base training**\n",
        "\n",
        "???"
      ],
      "metadata": {
        "id": "9rRpFEbisZky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def evaluate_stage1_base_top1(model: DFSLwF, loader: DataLoader, device) -> float:\n",
        "    model.fe.eval(); model.clf_base.eval()\n",
        "    correct, total = 0, 0\n",
        "    for xb, yb in loader:\n",
        "        xb = xb.to(device); yb = yb.to(device)\n",
        "        logits = model.clf_base(model.fe(xb))\n",
        "        pred = logits.argmax(dim=1)\n",
        "        correct += (pred == yb).sum().item()\n",
        "        total   += yb.numel()\n",
        "    return correct / max(1, total)"
      ],
      "metadata": {
        "id": "0LvUzNjVF-6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_stage1(model: DFSLwF, loader: DataLoader, device: torch.device,\n",
        "                 epochs: int = STAGE1_EPOCHS, lr: float = STAGE1_LR,\n",
        "                 weight_decay: float = STAGE1_WEIGHT_DECAY,\n",
        "                 val_loader: Optional[DataLoader] = None):\n",
        "\n",
        "    # [DFSLwF] Train feature extractor + base classifier (cosine)\n",
        "    model.fe.train(); model.clf_base.train(); model.gen.eval()\n",
        "\n",
        "    # Keep BN learnable here (paper trains a standard classifier in Stage-1)\n",
        "    params    = list(model.fe.parameters()) + list(model.clf_base.parameters())\n",
        "    opt       = torch.optim.Adam(params, lr=lr, weight_decay=weight_decay)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Params for validation\n",
        "    best_val = -1.0\n",
        "    patience = 0\n",
        "    best_state = None\n",
        "\n",
        "    with tqdm(range(epochs), desc=\"[Stage1] Supervised base training\") as epbar:\n",
        "        for ep in epbar:\n",
        "            model.fe.train(); model.clf_base.train()\n",
        "            batch_losses = []\n",
        "\n",
        "            for i, (xb, yb) in enumerate(loader):\n",
        "                xb = xb.to(device); yb = yb.to(device)\n",
        "                opt.zero_grad()\n",
        "                logits = model.clf_base(model.fe(xb))\n",
        "                loss   = criterion(logits, yb)\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "                batch_losses.append(float(loss.item()))\n",
        "\n",
        "                if (i + 1) % S1_LOG_EVERY == 0:\n",
        "                    epbar.set_postfix(loss=f\"{sliding_avg(batch_losses, S1_LOG_EVERY):.4f}\")\n",
        "\n",
        "            # ---- PERIODIC VALIDATION ----\n",
        "            if val_loader is not None and ((ep + 1) % S1_VAL_EVERY == 0):\n",
        "                val_acc = evaluate_stage1_base_top1(model, val_loader, device)\n",
        "                epbar.write(f\"\\t[stage1/val] epoch {ep+1:03d}: acc_base={100*val_acc:.2f}%\")\n",
        "\n",
        "                if val_acc > best_val + 1e-6:\n",
        "                    best_val   = val_acc\n",
        "                    patience   = 0\n",
        "                    best_state = {\n",
        "                        \"model\": copy.deepcopy(model.state_dict()),\n",
        "                        \"opt\":   copy.deepcopy(opt.state_dict()),\n",
        "                        \"epoch\": ep + 1,\n",
        "                        \"val\":   best_val,\n",
        "                    }\n",
        "                else:\n",
        "                    patience += 1\n",
        "                    if patience >= S1_PATIENCE:\n",
        "                        epbar.write(f\"[stage1] Early stopping (no improvement for {S1_PATIENCE} validations).\")\n",
        "                        break\n",
        "\n",
        "    # Restore best result\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state[\"model\"])"
      ],
      "metadata": {
        "id": "UKTZWOz4V6wh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = copy.deepcopy(baseline_model)\n",
        "train_stage1(model, train_loader_s1, device=device, val_loader=val_loader_s1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boHxe6JoviaS",
        "outputId": "19c77a7c-4733-4c5f-f16b-8a0ab1350d74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage1] Supervised base training:   2%|▏         | 2/120 [00:36<36:11, 18.40s/it, loss=3.2044]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage1/val] epoch 002: acc_base=12.91%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage1] Supervised base training:   3%|▎         | 4/120 [01:13<35:44, 18.48s/it, loss=2.5435]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage1/val] epoch 004: acc_base=19.34%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage1] Supervised base training:   5%|▌         | 6/120 [01:49<34:45, 18.29s/it, loss=2.1755]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage1/val] epoch 006: acc_base=30.34%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage1] Supervised base training:   7%|▋         | 8/120 [02:26<34:10, 18.31s/it, loss=1.8601]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage1/val] epoch 008: acc_base=35.31%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage1] Supervised base training:   8%|▊         | 10/120 [03:02<33:44, 18.40s/it, loss=1.6402]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage1/val] epoch 010: acc_base=45.06%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage1] Supervised base training:  10%|█         | 12/120 [03:40<33:28, 18.60s/it, loss=1.4634]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage1/val] epoch 012: acc_base=35.97%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage1] Supervised base training:  12%|█▏        | 14/120 [04:18<33:47, 19.13s/it, loss=1.3379]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage1/val] epoch 014: acc_base=45.06%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage1] Supervised base training:  13%|█▎        | 16/120 [04:56<32:59, 19.03s/it, loss=1.2211]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage1/val] epoch 016: acc_base=47.94%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage1] Supervised base training:  15%|█▌        | 18/120 [05:34<32:14, 18.96s/it, loss=1.1496]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage1/val] epoch 018: acc_base=32.06%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage1] Supervised base training:  17%|█▋        | 20/120 [06:11<31:42, 19.03s/it, loss=1.0355]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage1/val] epoch 020: acc_base=50.12%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage1] Supervised base training:  18%|█▊        | 22/120 [06:49<30:54, 18.93s/it, loss=0.9855]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage1/val] epoch 022: acc_base=57.66%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage1] Supervised base training:  20%|██        | 24/120 [07:27<30:18, 18.94s/it, loss=0.9150]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage1/val] epoch 024: acc_base=49.62%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage1] Supervised base training:  22%|██▏       | 26/120 [08:04<29:37, 18.91s/it, loss=0.8782]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage1/val] epoch 026: acc_base=53.06%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage1] Supervised base training:  23%|██▎       | 28/120 [08:41<28:50, 18.81s/it, loss=0.8353]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage1/val] epoch 028: acc_base=56.41%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage1] Supervised base training:  25%|██▌       | 30/120 [09:19<28:07, 18.75s/it, loss=0.7878]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage1/val] epoch 030: acc_base=53.84%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage1] Supervised base training:  27%|██▋       | 32/120 [09:56<27:25, 18.70s/it, loss=0.7713]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage1/val] epoch 032: acc_base=63.19%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage1] Supervised base training:  28%|██▊       | 34/120 [10:33<26:43, 18.64s/it, loss=0.7545]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage1/val] epoch 034: acc_base=53.84%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage1] Supervised base training:  30%|███       | 36/120 [11:10<26:05, 18.63s/it, loss=0.7252]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage1/val] epoch 036: acc_base=61.66%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage1] Supervised base training:  32%|███▏      | 38/120 [11:47<25:28, 18.64s/it, loss=0.7003]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage1/val] epoch 038: acc_base=60.09%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage1] Supervised base training:  33%|███▎      | 40/120 [12:24<24:51, 18.65s/it, loss=0.6819]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage1/val] epoch 040: acc_base=62.25%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage1] Supervised base training:  34%|███▍      | 41/120 [13:01<25:06, 19.07s/it, loss=0.6441]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage1/val] epoch 042: acc_base=62.06%\n",
            "\t[stage1] Early stopping (no improvement for 5 validations).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stage1_model = copy.deepcopy(model)"
      ],
      "metadata": {
        "id": "MTUp7LJb31fW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.2.2 Stage 2: episodic training**\n",
        "\n",
        "???"
      ],
      "metadata": {
        "id": "zYaBPBI4ssZq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def evaluate_gfsl(model: DFSLwF, test_loader: DataLoader, cifar_targets_all: List[int],\n",
        "                  base_classes: List[int], device: torch.device) -> Tuple[float, float, float]:\n",
        "    model.fe.eval(); model.clf_base.eval(); model.gen.eval()\n",
        "    acc_per_episode_base, acc_per_episode_novel = [], []\n",
        "    base_order = sorted(base_classes)\n",
        "    b2local = {cid: i for i, cid in enumerate(base_order)}\n",
        "    Cb = model.clf_base.weight.size(0)\n",
        "\n",
        "    for (support_novel, query_images, true_novel_ids, gt_novel, gt_base) in test_loader:\n",
        "        support_novel = support_novel.to(device)\n",
        "        query_images = query_images.to(device)\n",
        "        gt_novel = gt_novel.to(device)\n",
        "        gt_base = gt_base.to(device)\n",
        "\n",
        "        novel_weights = model.build_novel_weights(support_novel, K_SHOT)  # (N, D)\n",
        "        logits = model.forward_logits(query_images, novel_weights)\n",
        "        preds = logits.argmax(dim=1)\n",
        "\n",
        "        N = novel_weights.size(0)\n",
        "        pred_novel = preds[:N * Q_NOVEL] - Cb\n",
        "        pred_base = preds[N * Q_NOVEL:]\n",
        "\n",
        "        id2local = {cid: i for i, cid in enumerate(true_novel_ids)}\n",
        "        gt_novel_local = torch.tensor([id2local[int(y.item())] for y in gt_novel], device=device)\n",
        "        gt_base_local = torch.tensor([b2local[int(y.item())] for y in gt_base], device=device)\n",
        "\n",
        "        acc_b = (pred_base == gt_base_local).float().mean().item()\n",
        "        acc_n = (pred_novel == gt_novel_local).float().mean().item()\n",
        "        acc_per_episode_base.append(acc_b); acc_per_episode_novel.append(acc_n)\n",
        "\n",
        "    return len(acc_per_episode_base), gfsl_stats(acc_per_episode_base, acc_per_episode_novel)"
      ],
      "metadata": {
        "id": "Q0WCFplb3CNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_stage2(model: DFSLwF, meta_loader: DataLoader, device: torch.device,\n",
        "                 base_order: List[int], n_tasks: int = STAGE2_TASKS, val_every: int = STAGE2_VAL_EVERY,\n",
        "                 lr: float = STAGE2_LR, val_loader: Optional[DataLoader] = None):\n",
        "    \"\"\"\n",
        "    [DFSLwF] Freeze F, train generator + continue training W_base (and τ). Exclude pseudo-novel from attention memory.\n",
        "    Use *true* base labels for base queries; novel queries target indices are (Cb .. Cb+N-1).\n",
        "    \"\"\"\n",
        "    # Freeze feature extractor; freeze BN stats\n",
        "    model.fe.eval(); model.fe.apply(set_bn_eval)\n",
        "    for p in model.fe.parameters(): p.requires_grad = False\n",
        "\n",
        "    # Train generator, base weights, and tau\n",
        "    for p in model.clf_base.parameters(): p.requires_grad = True\n",
        "    params = list(model.gen.parameters()) + [model.clf_base.tau, model.clf_base.weight]\n",
        "    opt = torch.optim.Adam(params, lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # mapping CIFAR id -> local base index\n",
        "    b2local = {cid: i for i, cid in enumerate(sorted(base_order))}\n",
        "    Cb = model.clf_base.weight.size(0)\n",
        "\n",
        "    # Validation params\n",
        "    best_score = -1.0\n",
        "    patience   = 0\n",
        "    best_state = None\n",
        "\n",
        "    model.gen.train(); model.clf_base.train()\n",
        "    with tqdm(enumerate(meta_loader), total=len(meta_loader), desc=\"[Stage2] Episodic Training\") as pbar:\n",
        "        for step, (support_novel, query_images, true_novel_ids, base_q_labels_cifar) in pbar:\n",
        "            support_novel       = support_novel.to(device)\n",
        "            query_images        = query_images.to(device)\n",
        "            base_q_labels_cifar = base_q_labels_cifar.to(device)\n",
        "\n",
        "            # Mask: escludi pseudo-novel dall'attenzione\n",
        "            exclude_mask = torch.ones(Cb, dtype=torch.bool, device=device)\n",
        "            for cid in true_novel_ids:\n",
        "                if cid in b2local:\n",
        "                    exclude_mask[b2local[cid]] = False\n",
        "\n",
        "            # Novel weights (support -> gen), FE è congelato\n",
        "            with torch.no_grad():\n",
        "                supp_feats = model.fe(support_novel)\n",
        "            novel_weights = model.gen(supp_feats, model.clf_base.weight, K_SHOT, exclude_mask=exclude_mask)\n",
        "\n",
        "            # Logits e target\n",
        "            logits = model.forward_logits(query_images, novel_weights)  # [Cb | N]\n",
        "            N = novel_weights.size(0)\n",
        "\n",
        "            y_novel = torch.arange(N, device=device).repeat_interleave(Q_NOVEL)\n",
        "            targets = torch.empty(logits.size(0), dtype=torch.long, device=device)\n",
        "            targets[:N * Q_NOVEL] = Cb + y_novel\n",
        "            base_local = torch.tensor([b2local[int(y.item())] for y in base_q_labels_cifar], device=device)\n",
        "            targets[N * Q_NOVEL:] = base_local\n",
        "\n",
        "            opt.zero_grad()\n",
        "            loss = criterion(logits, targets)\n",
        "            loss.backward()\n",
        "            # torch.nn.utils.clip_grad_norm_(params, STAGE2_GRAD_CLIP)\n",
        "            opt.step()\n",
        "            pbar.set_postfix(loss=f\"{float(loss.item()):.4f}\")\n",
        "\n",
        "            # ---- PERIODIC VALIDATION ----\n",
        "            if val_loader is not None and ((step + 1) % val_every == 0):\n",
        "                Tval, vstats = evaluate_gfsl(model, val_loader, ds_train.targets, base_order, device)\n",
        "                v_base  = vstats[\"base\"][\"mean\"];  v_base_ci  = vstats[\"base\"][\"conf\"]\n",
        "                v_novel = vstats[\"novel\"][\"mean\"]; v_novel_ci = vstats[\"novel\"][\"conf\"]\n",
        "                v_h     = vstats[\"hmean\"][\"mean\"]; v_h_ci     = vstats[\"hmean\"][\"conf\"]\n",
        "\n",
        "                pbar.write(f\"\\t[stage2/val] step {step+1:05d}: \"\n",
        "                          f\"base={100*v_base:.2f}%±{100*v_base_ci:.2f}  \"\n",
        "                          f\"novel={100*v_novel:.2f}%±{100*v_novel_ci:.2f}  \"\n",
        "                          f\"h-mean={100*v_h:.2f}%±{100*v_h_ci:.2f}  (T={Tval})\")\n",
        "\n",
        "                sel = {\"base\": v_base, \"novel\": v_novel, \"hmean\": v_h}[S2_SELECT_METRIC]\n",
        "                if sel > best_score + 1e-6:\n",
        "                    best_score = sel\n",
        "                    patience   = 0\n",
        "                    best_state = {\n",
        "                        \"model\": copy.deepcopy(model.state_dict()),\n",
        "                        \"opt\":   copy.deepcopy(opt.state_dict()),\n",
        "                        \"step\":  step + 1,\n",
        "                        \"score\": best_score,\n",
        "                    }\n",
        "                else:\n",
        "                    patience += 1\n",
        "                    if patience >= S2_PATIENCE:\n",
        "                        pbar.write(f\"[stage2] Early stopping (no improvement for {S2_PATIENCE} validations).\")\n",
        "                        break\n",
        "\n",
        "                model.gen.train(); model.clf_base.train()\n",
        "\n",
        "    # Restore the best result\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state[\"model\"])"
      ],
      "metadata": {
        "id": "PmIOgTtNsmZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = copy.deepcopy(stage1_model)\n",
        "train_stage2(model, train_loader_s2, device=device, base_order=base_order, val_loader=val_loader_s2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffRMFtefvnma",
        "outputId": "04224011-73ab-4fe0-91e5-842d6be3048d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage2] Episodic Training:   3%|▎         | 502/20000 [01:32<37:31:25,  6.93s/it, loss=0.9299]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage2/val] step 00500: base=47.98%±0.38  novel=72.39%±0.52  h-mean=57.26%±0.31  (T=1000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage2] Episodic Training:   5%|▌         | 1003/20000 [03:05<31:42:53,  6.01s/it, loss=0.6792]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage2/val] step 01000: base=47.37%±0.38  novel=73.81%±0.50  h-mean=57.28%±0.32  (T=1000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage2] Episodic Training:   8%|▊         | 1502/20000 [04:37<30:48:50,  6.00s/it, loss=0.7881]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage2/val] step 01500: base=47.01%±0.39  novel=74.52%±0.52  h-mean=57.20%±0.33  (T=1000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage2] Episodic Training:  10%|█         | 2003/20000 [06:10<29:27:38,  5.89s/it, loss=0.8059]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage2/val] step 02000: base=47.03%±0.35  novel=74.77%±0.51  h-mean=57.37%±0.31  (T=1000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage2] Episodic Training:  13%|█▎        | 2502/20000 [07:40<29:32:09,  6.08s/it, loss=0.8211]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage2/val] step 02500: base=46.98%±0.38  novel=75.33%±0.51  h-mean=57.44%±0.32  (T=1000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage2] Episodic Training:  15%|█▌        | 3002/20000 [09:10<28:28:10,  6.03s/it, loss=0.8468]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage2/val] step 03000: base=46.83%±0.38  novel=74.90%±0.51  h-mean=57.20%±0.32  (T=1000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage2] Episodic Training:  18%|█▊        | 3503/20000 [10:48<29:20:39,  6.40s/it, loss=0.8594]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage2/val] step 03500: base=46.86%±0.37  novel=75.61%±0.50  h-mean=57.45%±0.31  (T=1000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage2] Episodic Training:  20%|██        | 4002/20000 [12:34<30:46:59,  6.93s/it, loss=0.7984]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage2/val] step 04000: base=46.54%±0.38  novel=75.35%±0.50  h-mean=57.12%±0.32  (T=1000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage2] Episodic Training:  23%|██▎       | 4502/20000 [14:10<29:33:17,  6.87s/it, loss=0.7996]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage2/val] step 04500: base=47.38%±0.39  novel=74.88%±0.51  h-mean=57.57%±0.32  (T=1000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage2] Episodic Training:  25%|██▌       | 5003/20000 [15:44<26:09:56,  6.28s/it, loss=0.7873]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage2/val] step 05000: base=47.52%±0.38  novel=74.71%±0.51  h-mean=57.66%±0.32  (T=1000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage2] Episodic Training:  28%|██▊       | 5503/20000 [17:21<23:25:10,  5.82s/it, loss=0.7184]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage2/val] step 05500: base=47.81%±0.37  novel=74.71%±0.52  h-mean=57.88%±0.31  (T=1000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage2] Episodic Training:  30%|███       | 6003/20000 [18:51<22:55:24,  5.90s/it, loss=0.7107]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage2/val] step 06000: base=47.81%±0.38  novel=75.24%±0.51  h-mean=58.05%±0.32  (T=1000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage2] Episodic Training:  33%|███▎      | 6502/20000 [20:22<23:02:21,  6.14s/it, loss=0.6854]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage2/val] step 06500: base=48.09%±0.39  novel=75.18%±0.53  h-mean=58.21%±0.33  (T=1000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage2] Episodic Training:  35%|███▌      | 7003/20000 [21:52<19:11:12,  5.31s/it, loss=0.6599]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage2/val] step 07000: base=47.56%±0.39  novel=75.01%±0.52  h-mean=57.74%±0.32  (T=1000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage2] Episodic Training:  38%|███▊      | 7503/20000 [23:23<20:13:13,  5.82s/it, loss=0.7589]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage2/val] step 07500: base=47.00%±0.39  novel=75.37%±0.50  h-mean=57.46%±0.33  (T=1000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage2] Episodic Training:  40%|████      | 8003/20000 [24:54<17:47:43,  5.34s/it, loss=0.7400]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage2/val] step 08000: base=47.62%±0.37  novel=74.94%±0.49  h-mean=57.86%±0.31  (T=1000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage2] Episodic Training:  43%|████▎     | 8502/20000 [26:25<21:31:23,  6.74s/it, loss=0.8193]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage2/val] step 08500: base=48.09%±0.38  novel=75.09%±0.50  h-mean=58.21%±0.32  (T=1000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage2] Episodic Training:  45%|████▌     | 9002/20000 [27:55<20:40:33,  6.77s/it, loss=0.8550]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage2/val] step 09000: base=48.49%±0.38  novel=75.03%±0.48  h-mean=58.50%±0.31  (T=1000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage2] Episodic Training:  48%|████▊     | 9502/20000 [29:25<17:43:55,  6.08s/it, loss=0.6644]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage2/val] step 09500: base=48.21%±0.38  novel=75.41%±0.52  h-mean=58.38%±0.32  (T=1000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage2] Episodic Training:  50%|█████     | 10002/20000 [30:55<16:40:15,  6.00s/it, loss=0.6931]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage2/val] step 10000: base=47.86%±0.37  novel=75.93%±0.52  h-mean=58.28%±0.31  (T=1000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage2] Episodic Training:  53%|█████▎    | 10502/20000 [32:25<16:11:31,  6.14s/it, loss=0.8324]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage2/val] step 10500: base=48.72%±0.36  novel=74.75%±0.51  h-mean=58.59%±0.31  (T=1000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage2] Episodic Training:  55%|█████▌    | 11003/20000 [33:56<13:23:33,  5.36s/it, loss=0.5748]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage2/val] step 11000: base=48.53%±0.38  novel=74.77%±0.51  h-mean=58.44%±0.32  (T=1000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage2] Episodic Training:  58%|█████▊    | 11502/20000 [35:25<14:25:20,  6.11s/it, loss=0.7183]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage2/val] step 11500: base=48.58%±0.38  novel=74.98%±0.51  h-mean=58.51%±0.31  (T=1000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage2] Episodic Training:  60%|██████    | 12002/20000 [36:55<13:32:00,  6.09s/it, loss=0.5841]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage2/val] step 12000: base=48.41%±0.38  novel=74.92%±0.49  h-mean=58.41%±0.32  (T=1000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage2] Episodic Training:  63%|██████▎   | 12502/20000 [38:28<13:21:54,  6.42s/it, loss=0.6418]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage2/val] step 12500: base=48.59%±0.39  novel=75.53%±0.50  h-mean=58.70%±0.32  (T=1000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage2] Episodic Training:  65%|██████▌   | 13003/20000 [40:00<10:39:05,  5.48s/it, loss=0.7062]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage2/val] step 13000: base=48.93%±0.39  novel=75.07%±0.51  h-mean=58.78%±0.31  (T=1000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage2] Episodic Training:  68%|██████▊   | 13502/20000 [41:30<12:10:17,  6.74s/it, loss=0.6671]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage2/val] step 13500: base=48.70%±0.38  novel=74.49%±0.51  h-mean=58.44%±0.30  (T=1000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage2] Episodic Training:  70%|███████   | 14002/20000 [42:59<10:05:16,  6.05s/it, loss=0.5749]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage2/val] step 14000: base=49.00%±0.37  novel=74.23%±0.51  h-mean=58.63%±0.31  (T=1000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage2] Episodic Training:  73%|███████▎  | 14502/20000 [44:29<9:20:44,  6.12s/it, loss=0.6961] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage2/val] step 14500: base=49.02%±0.38  novel=75.15%±0.50  h-mean=58.91%±0.31  (T=1000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage2] Episodic Training:  75%|███████▌  | 15002/20000 [45:59<9:24:10,  6.77s/it, loss=0.6869]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage2/val] step 15000: base=49.37%±0.37  novel=74.86%±0.52  h-mean=59.10%±0.31  (T=1000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage2] Episodic Training:  78%|███████▊  | 15502/20000 [47:28<8:26:16,  6.75s/it, loss=0.5240]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage2/val] step 15500: base=49.56%±0.37  novel=74.81%±0.52  h-mean=59.21%±0.32  (T=1000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage2] Episodic Training:  80%|████████  | 16002/20000 [48:57<6:43:32,  6.06s/it, loss=0.5591]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage2/val] step 16000: base=49.37%±0.38  novel=74.59%±0.51  h-mean=58.98%±0.31  (T=1000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage2] Episodic Training:  83%|████████▎ | 16503/20000 [50:30<5:57:31,  6.13s/it, loss=0.5547] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage2/val] step 16500: base=49.65%±0.37  novel=75.05%±0.50  h-mean=59.36%±0.31  (T=1000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage2] Episodic Training:  85%|████████▌ | 17002/20000 [52:05<6:10:02,  7.41s/it, loss=0.5431]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage2/val] step 17000: base=49.45%±0.36  novel=74.81%±0.50  h-mean=59.16%±0.30  (T=1000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage2] Episodic Training:  88%|████████▊ | 17502/20000 [53:40<5:07:23,  7.38s/it, loss=0.5720]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage2/val] step 17500: base=48.93%±0.37  novel=74.53%±0.52  h-mean=58.66%±0.32  (T=1000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage2] Episodic Training:  90%|█████████ | 18003/20000 [55:15<2:56:14,  5.30s/it, loss=0.6514]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage2/val] step 18000: base=49.26%±0.37  novel=74.45%±0.52  h-mean=58.89%±0.32  (T=1000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage2] Episodic Training:  93%|█████████▎| 18503/20000 [56:45<2:07:49,  5.12s/it, loss=0.7167]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage2/val] step 18500: base=49.51%±0.38  novel=74.49%±0.51  h-mean=59.07%±0.32  (T=1000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage2] Episodic Training:  95%|█████████▌| 19002/20000 [58:14<1:39:09,  5.96s/it, loss=0.5008]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage2/val] step 19000: base=49.95%±0.37  novel=74.40%±0.51  h-mean=59.37%±0.31  (T=1000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage2] Episodic Training:  98%|█████████▊| 19502/20000 [59:43<55:28,  6.68s/it, loss=0.6629]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage2/val] step 19500: base=49.70%±0.38  novel=74.49%±0.51  h-mean=59.22%±0.32  (T=1000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage2] Episodic Training: 100%|██████████| 20000/20000 [1:01:12<00:00,  5.45it/s, loss=0.6459]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage2/val] step 20000: base=49.89%±0.37  novel=74.80%±0.52  h-mean=59.46%±0.32  (T=1000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.3 Evaluation**\n",
        "\n",
        "???"
      ],
      "metadata": {
        "id": "DfsB1tdtsyKO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "T, stats = evaluate_gfsl(stage1_model, test_loader, ds_test.targets, base, device=device)\n",
        "print_stats(T, stats, model=\"After stage 1 training\")"
      ],
      "metadata": {
        "id": "4saf_h3mUYfm",
        "outputId": "f0cb2855-ce2c-45ad-b22b-aa1c08a37073",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[test] - After stage 1 training (95% CI on 1000 tasks)\n",
            " - [Base]   acc=57.54% ± 0.36%\n",
            " - [Novel]  acc=51.71% ± 0.51%\n",
            " - [H-mean] acc=53.97% ± 0.33%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "T, stats = evaluate_gfsl(model, test_loader, ds_test.targets, base, device=device)\n",
        "print_stats(T, stats, model=\"After stage 2 training\")"
      ],
      "metadata": {
        "id": "71N0971Mvqm6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5329c8ae-0fc9-4c82-e29a-62856355f455"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[test] - After stage 2 training (95% CI on 1000 tasks)\n",
            " - [Base]   acc=49.93% ± 0.38%\n",
            " - [Novel]  acc=71.31% ± 0.53%\n",
            " - [H-mean] acc=58.28% ± 0.31%\n"
          ]
        }
      ]
    }
  ]
}