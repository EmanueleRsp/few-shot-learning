{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Generalized Few-Shot Learning (GFSL)**\n",
        "\n",
        "Intro"
      ],
      "metadata": {
        "id": "tFoEk1MspnUb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "PgPkCPJxpl63"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Background**"
      ],
      "metadata": {
        "id": "CFGadMzopx5K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.1 Generalized Few-Shot Leaning (GFSL)**\n",
        "\n",
        "???"
      ],
      "metadata": {
        "id": "tiLuW00wp55a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.2 Dynamic Few-Shot Learning without Forgetting**\n",
        "\n",
        "???"
      ],
      "metadata": {
        "id": "ZJGmdC0np5IB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.3 CIFAR100 dataset**\n",
        "\n",
        "???"
      ],
      "metadata": {
        "id": "5CzeVUAMqNiN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "AXRqQlJspyNl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Practice**"
      ],
      "metadata": {
        "id": "UAwYkSFHoebf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "import math\n",
        "import random\n",
        "import types\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "import copy\n",
        "from functools import partial\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import ConcatDataset, DataLoader, Subset\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import CIFAR100\n",
        "from torchvision.models import resnet18, ResNet18_Weights\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "0c0fLQwq5t_4"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seed:"
      ],
      "metadata": {
        "id": "nlrbjXRRoo-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 42\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "train_rng = np.random.default_rng(SEED + 1)\n",
        "test_rng = np.random.default_rng(SEED + 2)"
      ],
      "metadata": {
        "id": "h0QZmdPV52XF"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Settings:"
      ],
      "metadata": {
        "id": "_6NNqLzFoxUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Few-shot / episodic config (Stage 2 + Test)\n",
        "N_WAY = 5\n",
        "K_SHOT = 5\n",
        "Q_NOVEL = 15\n",
        "Q_BASE_TOTAL = 75\n",
        "\n",
        "# CIFAR-100 split sizes\n",
        "N_BASE = 64\n",
        "N_VALNOVEL = 16\n",
        "N_TESTNOVEL = 20\n",
        "\n",
        "# Network params\n",
        "TAU_INIT = 10.0         # temperature init\n",
        "BACKBONE = \"resnet18\"   # feature extractor network: \"resnet18\" or \"conv\"\n",
        "assert BACKBONE in [\"resnet18\", \"conv\"], \"BACKBONE must be in ['resnet18', 'conv']\"\n",
        "\n",
        "# Stage 1\n",
        "STAGE1_EPOCHS = 120\n",
        "STAGE1_LR = 3e-3\n",
        "STAGE1_BS = 512\n",
        "STAGE1_WEIGHT_DECAY = 5e-4\n",
        "STAGE1_LABEL_SMOOTH = 0.1\n",
        "S1_VAL_FRAC   = 0.10   # frazione per classe da tenere da parte per la validazione\n",
        "S1_VAL_EVERY  = 2      # valida ogni N epoche\n",
        "S1_PATIENCE   = 5      # early stop dopo N validazioni senza migliorare\n",
        "S1_LOG_EVERY  = 50     # log loss ogni N batch\n",
        "\n",
        "# Stage 2\n",
        "STAGE2_TASKS = 20_000\n",
        "STAGE2_VAL_EVERY = 500\n",
        "STAGE2_LR = 5e-4\n",
        "STAGE2_GRAD_CLIP = 1.0\n",
        "S2_VAL_TASKS      = 1_000    # #episodi usati ad ogni validazione Stage-2\n",
        "S2_PATIENCE       = 5        # early-stopping (GFSL) dopo N validazioni senza miglioramenti\n",
        "S2_SELECT_METRIC  = \"hmean\"  # metrica per scegliere il best (hmean|base|novel)\n",
        "\n",
        "# Test\n",
        "TEST_TASKS = 1_000"
      ],
      "metadata": {
        "id": "MvrrIQjKoxLe"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utility functions:"
      ],
      "metadata": {
        "id": "NOTdCwcdpGaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_cifar100_classes(seed: int, n_base=64, n_val=16, n_test=20):\n",
        "    \"\"\"Split CIFAR-100 class IDs into base/val-novel/test-novel sets.\n",
        "\n",
        "    Shuffles the 100 class IDs with a reproducible RNG and returns three\n",
        "    disjoint lists for base classes (used for supervised training), validation\n",
        "    novel classes (optional episodic validation), and test novel classes\n",
        "    (used in GFSL evaluation).\n",
        "\n",
        "    Args:\n",
        "        seed: Random seed for the class shuffling.\n",
        "        n_base: Number of base classes.\n",
        "        n_val: Number of validation novel classes.\n",
        "        n_test: Number of test novel classes.\n",
        "\n",
        "    Returns:\n",
        "        A tuple (base, valn, testn) where each element is a list of class IDs.\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "    classes = np.arange(100); rng.shuffle(classes)\n",
        "    return classes[:n_base].tolist(), classes[n_base:n_base+n_val].tolist(), classes[n_base+n_val:n_base+n_val+n_test].tolist()\n",
        "\n",
        "def subset_by_classes(ds, keep):\n",
        "    \"\"\"Return a Subset containing only samples whose label is in `keep`.\n",
        "\n",
        "    Uses vectorized filtering over `ds.targets` to select the indices that\n",
        "    belong to the provided set of class IDs.\n",
        "\n",
        "    Args:\n",
        "        ds: A torchvision-style dataset exposing `targets` (list/array of ints).\n",
        "        keep: Iterable of class IDs to retain.\n",
        "\n",
        "    Returns:\n",
        "        torch.utils.data.Subset wrapping `ds` with filtered indices.\n",
        "    \"\"\"\n",
        "    t = np.array(ds.targets)\n",
        "    idx = np.nonzero(np.isin(t, keep))[0]\n",
        "    return Subset(ds, idx)\n",
        "\n",
        "def class_to_local_indices(subset):\n",
        "    \"\"\"Build a mapping class_id -> list of *local* indices within `subset`.\n",
        "\n",
        "    Iterates over the subset indices and groups them by their original class\n",
        "    ID (read from `subset.dataset.targets`). Useful for fast episodic sampling\n",
        "    (e.g., drawing K support and Q query images per class).\n",
        "\n",
        "    Args:\n",
        "        subset: A torch Subset whose `dataset` exposes `targets`\n",
        "            and whose `indices` reference the original dataset.\n",
        "\n",
        "    Returns:\n",
        "        Dict[int, List[int]] mapping each class ID to a list of local indices\n",
        "        (0..len(subset)-1) within the subset.\n",
        "\n",
        "    Notes:\n",
        "        The returned indices are local to `subset` (not the original dataset).\n",
        "    \"\"\"\n",
        "    t = np.array(subset.dataset.targets)\n",
        "    out = {}\n",
        "    for j, i in enumerate(subset.indices):\n",
        "        y = int(t[i])\n",
        "        (out.setdefault(y, [])).append(j)\n",
        "    return out\n",
        "\n",
        "def stratified_split_subset(subset: Subset, val_frac: float, seed: int):\n",
        "    \"\"\"\n",
        "    Divide un Subset (stesse classi) in (train_part, val_part),\n",
        "    mantenendo proporzioni per ciascuna classe CIFAR originale.\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "    ds_targets = np.array(subset.dataset.targets)\n",
        "    # mappa: class_id -> lista di indici LOCALi nel subset\n",
        "    cls2locals = {}\n",
        "    for j, i in enumerate(subset.indices):\n",
        "        y = int(ds_targets[i])\n",
        "        (cls2locals.setdefault(y, [])).append(j)\n",
        "\n",
        "    train_locals, val_locals = [], []\n",
        "    for y, locals_ in cls2locals.items():\n",
        "        locals_ = np.array(locals_, dtype=int)\n",
        "        n_val = max(1, int(round(len(locals_) * val_frac)))\n",
        "        rng.shuffle(locals_)\n",
        "        val_locals.append(locals_[:n_val])\n",
        "        train_locals.append(locals_[n_val:])\n",
        "\n",
        "    train_locals = np.concatenate(train_locals).tolist()\n",
        "    val_locals   = np.concatenate(val_locals).tolist()\n",
        "\n",
        "    train_indices = [subset.indices[i] for i in train_locals]\n",
        "    val_indices   = [subset.indices[i] for i in val_locals]\n",
        "\n",
        "    return Subset(subset.dataset, train_indices), Subset(subset.dataset, val_indices)"
      ],
      "metadata": {
        "id": "bRJokFSoC3Wo"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def l2_normalize(x: torch.Tensor, dim: int = 1, eps: float = 1e-6) -> torch.Tensor:\n",
        "  \"\"\"L2-normalize a tensor along a given dimension.\n",
        "\n",
        "  Each vector along `dim` is divided by its L2 norm, producing unit-length\n",
        "  vectors. A small epsilon is used to avoid division by zero.\n",
        "\n",
        "  Args:\n",
        "      x: Input tensor.\n",
        "      dim: Dimension along which to compute the L2 norm (default: 1).\n",
        "      eps: Minimum norm value used for numerical stability (default: 1e-6).\n",
        "\n",
        "  Returns:\n",
        "      A tensor with the same shape as `x`, L2-normalized along `dim`.\n",
        "  \"\"\"\n",
        "  return x / (x.norm(p=2, dim=dim, keepdim=True).clamp_min(eps))"
      ],
      "metadata": {
        "id": "g6FYkq_npGJR"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_bn_eval(m: nn.Module):\n",
        "  \"\"\"Put BatchNorm2d layers in eval mode and freeze their parameters.\n",
        "\n",
        "  When applied (e.g., `model.apply(set_bn_eval)`), this sets each\n",
        "  `nn.BatchNorm2d` module to evaluation mode so it uses stored running\n",
        "  statistics and stops updating them, and it disables gradient updates\n",
        "  for its affine parameters (gamma/beta).\n",
        "\n",
        "  Args:\n",
        "      m: A module that may be an instance of `nn.BatchNorm2d`.\n",
        "\n",
        "  Returns:\n",
        "      None. The module is modified in place if it is BatchNorm2d.\n",
        "  \"\"\"\n",
        "  if isinstance(m, nn.BatchNorm2d):\n",
        "      m.eval()\n",
        "      for p in m.parameters():\n",
        "          p.requires_grad = False"
      ],
      "metadata": {
        "id": "nh0Z36WvpJ1j"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sliding_avg(xs: List[float], k: int = 20) -> float:\n",
        "  if not xs:\n",
        "      return 0.0\n",
        "  if len(xs) < k:\n",
        "      return float(sum(xs) / len(xs))\n",
        "  return float(sum(xs[-k:]) / k)"
      ],
      "metadata": {
        "id": "aXkLYzad5_UO"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_ci95(xs: np.ndarray) -> Tuple[float, float]:\n",
        "    xs = np.asarray(xs, dtype=float)\n",
        "    n  = xs.size\n",
        "    if n < 2:\n",
        "        return float(xs.mean()), 0.0\n",
        "    std = xs.std(ddof=1)\n",
        "    stderr = std / np.sqrt(n)\n",
        "    z = 1.96\n",
        "    return float(xs.mean()), float(z * stderr)\n",
        "\n",
        "def gfsl_stats(\n",
        "    acc_per_ep_base: List[float],\n",
        "    acc_per_ep_novel: List[float],\n",
        ") -> Dict[str, Dict[str, float]]:\n",
        "\n",
        "    if len(acc_per_ep_base) != len(acc_per_ep_novel):\n",
        "        raise ValueError(\"base and novel must be of the same length\")\n",
        "    T = len(acc_per_ep_base)\n",
        "\n",
        "    base = np.asarray(acc_per_ep_base, dtype=float)\n",
        "    novel = np.asarray(acc_per_ep_novel, dtype=float)\n",
        "\n",
        "    denom = base + novel\n",
        "    h_per_ep = np.where(denom > 0, 2.0 * base * novel / denom, 0.0)\n",
        "\n",
        "    base_mean, base_ci  = mean_ci95(base)\n",
        "    novel_mean, novel_ci = mean_ci95(novel)\n",
        "    h_mean, h_ci = mean_ci95(h_per_ep)\n",
        "\n",
        "    return {\n",
        "        \"base\":  {\"mean\": base_mean,  \"conf\": base_ci},\n",
        "        \"novel\": {\"mean\": novel_mean, \"conf\": novel_ci},\n",
        "        \"hmean\": {\"mean\": h_mean,     \"conf\": h_ci},\n",
        "    }\n",
        "\n",
        "def print_stats(T: int, stats: Dict[str, Dict[str, float]], model: str = \"\"):\n",
        "  print(f\"[test] - {model} (95% CI on {T} tasks)\")\n",
        "  print(f\" - [Base]   acc={100*stats['base']['mean']:.2f}% ± {100*stats['base']['conf']:.2f}%\")\n",
        "  print(f\" - [Novel]  acc={100*stats['novel']['mean']:.2f}% ± {100*stats['novel']['conf']:.2f}%\")\n",
        "  print(f\" - [H-mean] acc={100*stats['hmean']['mean']:.2f}% ± {100*stats['hmean']['conf']:.2f}%\")\n"
      ],
      "metadata": {
        "id": "RWvm92IonBf3"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.1 Environment**"
      ],
      "metadata": {
        "id": "aRvCumaepV4w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.1.1 CIFAR100 dataset**\n",
        "\n",
        "???"
      ],
      "metadata": {
        "id": "NMpMP2wfpYWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if BACKBONE == \"conv\":\n",
        "    IMAGE_SIZE = 32\n",
        "\n",
        "    train_tf = transforms.Compose([\n",
        "        transforms.RandomCrop(IMAGE_SIZE, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "    eval_tf = transforms.Compose([transforms.ToTensor(),])\n",
        "\n",
        "else:\n",
        "    IMNET_MEAN = [0.485, 0.456, 0.406]\n",
        "    IMNET_STD  = [0.229, 0.224, 0.225]\n",
        "    IM_RESIZE = 128\n",
        "    IM_CROP   = 112\n",
        "\n",
        "    train_tf = transforms.Compose([\n",
        "        transforms.Resize(IM_RESIZE),\n",
        "        transforms.RandomCrop(IM_CROP),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(IMNET_MEAN, IMNET_STD),\n",
        "    ])\n",
        "\n",
        "    eval_tf = transforms.Compose([\n",
        "        transforms.Resize(IM_RESIZE),\n",
        "        transforms.CenterCrop(IM_CROP),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(IMNET_MEAN, IMNET_STD),\n",
        "    ])\n"
      ],
      "metadata": {
        "id": "HwaG9qKI57p6"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_train = CIFAR100(root=\"./data\", train=True,  transform=train_tf, download=True)\n",
        "ds_test  = CIFAR100(root=\"./data\", train=False, transform=eval_tf,  download=True)\n",
        "\n",
        "base, valn, testn = split_cifar100_classes(SEED)"
      ],
      "metadata": {
        "id": "QcTRsE73tYyU"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stage 1: Train + validation (base) / Stage 2: Train (base)\n",
        "train_base_full = subset_by_classes(ds_train, base)\n",
        "train_base_tr, train_base_val = stratified_split_subset(train_base_full, S1_VAL_FRAC, SEED+7)\n",
        "\n",
        "# Stage 2: Validation (Novel)\n",
        "train_valnovel = subset_by_classes(ds_train, valn)\n",
        "\n",
        "# Test (base + novel)\n",
        "test_base  = subset_by_classes(ds_test,  base)\n",
        "test_novel = subset_by_classes(ds_test,  testn)"
      ],
      "metadata": {
        "id": "Zxqc9nIwBUTq"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Per Stage-1 (supervised)\n",
        "cti_train_base_tr  = class_to_local_indices(train_base_tr)\n",
        "cti_train_base_val = class_to_local_indices(train_base_val)\n",
        "\n",
        "# Per Stage-2 VALIDAZIONE (episodica GFSL) — tutto dal TRAIN\n",
        "cti_val_base   = cti_train_base_val\n",
        "cti_val_novel  = class_to_local_indices(train_valnovel)\n",
        "\n",
        "# Per TEST (episodico GFSL) — come prima\n",
        "cti_test_base  = class_to_local_indices(test_base)\n",
        "cti_test_novel = class_to_local_indices(test_novel)"
      ],
      "metadata": {
        "id": "TTGwCpCPBRPM"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.1.1.1 DataLoader: Stage 1**"
      ],
      "metadata": {
        "id": "m0uFfFH8tbjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Stage1TrainDS(torch.utils.data.Dataset):\n",
        "    def __init__(self, subset: Subset, orig_targets: List[int], order_map: Dict[int, int]):\n",
        "        \"\"\"Subset over base classes con label locali precompute [0..Cb-1].\"\"\"\n",
        "        self.subset = subset\n",
        "        self.local_labels = [order_map[int(orig_targets[i])] for i in subset.indices]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.subset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x, _ = self.subset[idx]\n",
        "        return x, self.local_labels[idx]"
      ],
      "metadata": {
        "id": "bIui-IjnGOfr"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_order = sorted(base)\n",
        "order_map  = {cid: i for i, cid in enumerate(base_order)}\n",
        "\n",
        "stage1_train_ds = Stage1TrainDS(train_base_tr,  ds_train.targets, order_map)  # train\n",
        "stage1_val_ds   = Stage1TrainDS(train_base_val, ds_train.targets, order_map)  # val (disjoined)\n",
        "\n",
        "train_loader_s1 = DataLoader(stage1_train_ds, batch_size=STAGE1_BS, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader_s1   = DataLoader(stage1_val_ds,   batch_size=STAGE1_BS*2, shuffle=False, num_workers=2, pin_memory=True)"
      ],
      "metadata": {
        "id": "7UGqx2oStiVD"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.1.1.2 DataLoader: Stage 2**"
      ],
      "metadata": {
        "id": "yGENDGSFtivp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GFSLTrainEpisodicBatchSampler:\n",
        "    \"\"\"\n",
        "    Stage-2: N_WAY pseudo-novel (from BASE train) with K+Qn each + Qb base queries from BASE (any class).\n",
        "    Returns indices over the Subset(train_base).\n",
        "    \"\"\"\n",
        "    def __init__(self, class_to_indices: Dict[int, List[int]], n_tasks: int, rng: np.random.Generator,\n",
        "                 n_way: int = N_WAY, k_shot: int = K_SHOT, q_novel: int = Q_NOVEL, q_base_total: int = Q_BASE_TOTAL):\n",
        "        self.cti = class_to_indices\n",
        "        self.all_classes = list(class_to_indices.keys())\n",
        "        self.n_tasks = n_tasks\n",
        "        self.rng = rng\n",
        "        self.n_way = n_way\n",
        "        self.k_shot = k_shot\n",
        "        self.q_novel = q_novel\n",
        "        self.q_base_total = q_base_total\n",
        "\n",
        "    def __len__(self): return self.n_tasks\n",
        "\n",
        "    def __iter__(self):\n",
        "        for _ in range(self.n_tasks):\n",
        "\n",
        "            novel = self.rng.choice(self.all_classes, size=self.n_way, replace=False)\n",
        "\n",
        "            batch = []\n",
        "            for c in novel:\n",
        "                pool = self.cti[c]\n",
        "                need = self.k_shot + self.q_novel\n",
        "                if len(pool) < need:\n",
        "                    raise ValueError(f\"Class {c} has {len(pool)} < {need}\")\n",
        "                idx = self.rng.choice(pool, size=need, replace=False)\n",
        "                batch.append(idx)\n",
        "\n",
        "            novel_classes = set(novel.tolist())\n",
        "            base_pool_classes = [c for c in self.all_classes if c not in novel_classes]\n",
        "\n",
        "            used = set(np.concatenate(batch).tolist())\n",
        "\n",
        "            base_q = []\n",
        "            while len(base_q) < self.q_base_total:\n",
        "                c = int(self.rng.choice(base_pool_classes))\n",
        "                cand = int(self.rng.choice(self.cti[c]))\n",
        "                if cand not in used:\n",
        "                    base_q.append(cand)\n",
        "                    used.add(cand)\n",
        "\n",
        "            batch.append(np.array(base_q, dtype=int))\n",
        "            yield np.concatenate(batch)\n",
        "\n",
        "stage2_train_batch_sampler = GFSLTrainEpisodicBatchSampler(\n",
        "    cti_train_base_tr, n_tasks=STAGE2_TASKS, rng=train_rng,\n",
        "    n_way=N_WAY, k_shot=K_SHOT, q_novel=Q_NOVEL, q_base_total=Q_BASE_TOTAL\n",
        ")"
      ],
      "metadata": {
        "id": "jWdAwrxL6KH-"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stage2_collate(batch, n_way: int, k_shot: int, q_novel: int, q_base_total: int):\n",
        "    \"\"\"\n",
        "    Output:\n",
        "      - support_novel: (N*K, C, H, W)\n",
        "      - query_images : (N*Qn + Qb, C, H, W)\n",
        "      - true_novel_ids (original CIFAR ids, one per novel class)\n",
        "      - base_query_labels_cifar: (Qb,) original CIFAR ids for base queries (for true targets)\n",
        "    \"\"\"\n",
        "    imgs, labs = list(zip(*batch))\n",
        "    images = torch.stack(imgs)\n",
        "    labels = torch.tensor([int(y) for y in labs])\n",
        "\n",
        "    per_novel = k_shot + q_novel\n",
        "    total_novel = n_way * per_novel\n",
        "\n",
        "    novel_block = images[:total_novel].view(n_way, per_novel, *images.shape[1:])\n",
        "    novel_labels_block = labels[:total_novel].view(n_way, per_novel)\n",
        "\n",
        "    support_novel = novel_block[:, :k_shot].reshape(-1, *images.shape[1:])\n",
        "    query_novel = novel_block[:, k_shot:].reshape(-1, *images.shape[1:])\n",
        "    query_base = images[total_novel:]\n",
        "\n",
        "    query_images = torch.cat([query_novel, query_base], dim=0)\n",
        "\n",
        "    true_novel_ids = [int(novel_labels_block[i, 0].item()) for i in range(n_way)]\n",
        "    base_query_labels_cifar = labels[total_novel:]  # original CIFAR ids\n",
        "    return support_novel, query_images, true_novel_ids, base_query_labels_cifar\n",
        "\n",
        "stage2_collate_fn = partial(\n",
        "    stage2_collate,\n",
        "    n_way=N_WAY, k_shot=K_SHOT, q_novel=Q_NOVEL, q_base_total=Q_BASE_TOTAL,\n",
        ")"
      ],
      "metadata": {
        "id": "VKDRe5296RFa"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Stage2TrainDS(torch.utils.data.Dataset):\n",
        "    def __init__(self, subset: Subset):\n",
        "        self.subset = subset\n",
        "    def __len__(self):\n",
        "      return len(self.subset)\n",
        "    def __getitem__(self, idx):\n",
        "      return self.subset[idx]\n",
        "\n",
        "stage2_train_ds = Stage2TrainDS(train_base_tr)"
      ],
      "metadata": {
        "id": "zyl7POdxWpoE"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader_s2 = DataLoader(\n",
        "    stage2_train_ds, batch_sampler=stage2_train_batch_sampler,\n",
        "    collate_fn=stage2_collate_fn, num_workers=2, pin_memory=True\n",
        ")"
      ],
      "metadata": {
        "id": "0-sHBaXruHqM"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.1.1.3 DataLoader: Stage 2 Validation and Test**"
      ],
      "metadata": {
        "id": "mxGopBG5u_6A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GFSLEvalEpisodeSampler:\n",
        "    \"\"\"Batch sampler for GFSL test episodes over a single ConcatDataset.\n",
        "\n",
        "    Each yielded batch is a 1D numpy array of indices into `test_concat`\n",
        "    laid out as:\n",
        "        [ N_WAY*(K_SHOT+Q_NOVEL) indices from novel part | Q_BASE_TOTAL indices from base part ]\n",
        "\n",
        "    This mirrors Stage-2 structure, but draws from the *test* splits:\n",
        "      - support + query for true novel classes from the novel test subset\n",
        "      - base queries from the base test subset\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, base_cti: Dict[int, List[int]], novel_cti: Dict[int, List[int]], offset_base: int, n_tasks: int, rng: np.random.Generator,\n",
        "                 n_way: int = N_WAY, k_shot: int = K_SHOT, q_novel: int = Q_NOVEL, q_base_total: int = Q_BASE_TOTAL):\n",
        "        # class-id -> list of local indices (within each Subset) for base/novel test splits\n",
        "        self.base_cti = base_cti\n",
        "        self.novel_cti = novel_cti\n",
        "        self.offset_base = offset_base\n",
        "\n",
        "        # explicit class-id pools\n",
        "        self.base_classes  = list(base_cti.keys())\n",
        "        self.novel_classes = list(novel_cti.keys())\n",
        "\n",
        "        # episode config\n",
        "        self.n_tasks = n_tasks\n",
        "        self.rng = rng\n",
        "        self.n_way = n_way\n",
        "        self.k_shot = k_shot\n",
        "        self.q_novel = q_novel\n",
        "        self.q_base_total = q_base_total\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_tasks\n",
        "\n",
        "    def __iter__(self):\n",
        "        for _ in range(self.n_tasks):\n",
        "            # ---- NOVEL BLOCK: pick N_WAY true novel classes and sample K+Qn per class (no replacement) ----\n",
        "            chosen_novel = self.rng.choice(self.novel_classes, size=self.n_way, replace=False)\n",
        "\n",
        "            novel_chunks = []\n",
        "            per_novel = self.k_shot + self.q_novel\n",
        "\n",
        "            for c in chosen_novel:\n",
        "                pool = self.novel_cti[c]                 # local indices within test_novel\n",
        "                if len(pool) < per_novel:\n",
        "                    raise ValueError(f\"Novel class {c} has {len(pool)} < {per_novel}\")\n",
        "                # sample K+Qn *without* replacement to avoid reusing the same image as support/query\n",
        "                idx = self.rng.choice(pool, size=per_novel, replace=False)\n",
        "                novel_chunks.append(idx)\n",
        "\n",
        "            # Flatten the novel part; indices are still in the \"novel namespace\" (no offset)\n",
        "            novel_block = np.concatenate(novel_chunks).astype(int)\n",
        "\n",
        "            # ---- BASE BLOCK: sample Qb indices from base part (optionally allow replacement) ----\n",
        "            base_q = []\n",
        "            while len(base_q) < self.q_base_total:\n",
        "                c = int(self.rng.choice(self.base_classes))\n",
        "                # sample a local index within test_base\n",
        "                cand_local = int(self.rng.choice(self.base_cti[c]))\n",
        "                # shift to address the second component of ConcatDataset\n",
        "                base_q.append(cand_local + self.offset_base)\n",
        "\n",
        "            base_block = np.array(base_q, dtype=int)\n",
        "\n",
        "            # ---- FINAL EPISODE ----\n",
        "            # Concatenate [novel | base] to match the downstream collate expectations\n",
        "            full_episode = np.concatenate([novel_block, base_block])\n",
        "            yield full_episode"
      ],
      "metadata": {
        "id": "V1IuPKZRVVzL"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_collate(batch, n_way: int, k_shot: int, q_novel: int, q_base_total: int):\n",
        "    \"\"\"Reconstruct support/query tensors for a GFSL test episode.\n",
        "\n",
        "    Input `batch` is a list of (image, label) from `test_concat` where the first\n",
        "    N_WAY*(K_SHOT+Q_NOVEL) items belong to the novel subset and the remaining Q_BASE_TOTAL\n",
        "    items belong to the base subset.\n",
        "    \"\"\"\n",
        "    imgs, labs = list(zip(*batch))\n",
        "    images = torch.stack(imgs)\n",
        "    labels = torch.tensor([int(y) for y in labs])\n",
        "\n",
        "    per_novel = k_shot + q_novel\n",
        "    total_novel = n_way * per_novel\n",
        "\n",
        "    # reshape novel block into (N, K+Qn, C, H, W) and (N, K+Qn) for labels\n",
        "    novel_block = images[:total_novel].view(n_way, per_novel, *images.shape[1:])\n",
        "    novel_labels_block = labels[:total_novel].view(n_way, per_novel)\n",
        "\n",
        "    # split into support (first K) and query (last Qn)\n",
        "    support_novel = novel_block[:, :k_shot].reshape(-1, *images.shape[1:])\n",
        "    query_novel   = novel_block[:, k_shot:].reshape(-1, *images.shape[1:])\n",
        "    query_base    = images[total_novel:]  # remaining Qb from base subset\n",
        "\n",
        "    # concatenate all queries [novel | base]\n",
        "    query_images = torch.cat([query_novel, query_base], dim=0)\n",
        "\n",
        "    # collect true novel CIFAR IDs (one per class) and per-query GT labels\n",
        "    true_novel_ids = [int(novel_labels_block[i, 0].item()) for i in range(n_way)]\n",
        "    gt_novel = novel_labels_block[:, k_shot:].reshape(-1)\n",
        "    gt_base  = labels[total_novel:]\n",
        "\n",
        "    return support_novel, query_images, true_novel_ids, gt_novel, gt_base\n",
        "\n",
        "eval_collate_fn = partial(\n",
        "    eval_collate,\n",
        "    n_way=N_WAY, k_shot=K_SHOT, q_novel=Q_NOVEL, q_base_total=Q_BASE_TOTAL,\n",
        ")"
      ],
      "metadata": {
        "id": "PmIR_s8bVaDf"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validation:"
      ],
      "metadata": {
        "id": "t6ylrgepIMi3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampler_val_s2 = GFSLEvalEpisodeSampler(\n",
        "    cti_val_base, cti_val_novel, len(train_valnovel),\n",
        "    n_tasks=S2_VAL_TASKS, rng=test_rng,\n",
        "    n_way=N_WAY, k_shot=K_SHOT, q_novel=Q_NOVEL, q_base_total=Q_BASE_TOTAL\n",
        ")"
      ],
      "metadata": {
        "id": "jWw60OHxI8gv"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a single test dataset by concatenating the two subsets.\n",
        "# Order matters: novel part first, base part second.\n",
        "val_concat_s2 = ConcatDataset([train_valnovel, train_base_val])"
      ],
      "metadata": {
        "id": "iveYcIVtIeoy"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_loader_s2 = DataLoader(\n",
        "    val_concat_s2,\n",
        "    batch_sampler=sampler_val_s2,\n",
        "    collate_fn=eval_collate_fn,\n",
        "    num_workers=2,\n",
        "    pin_memory=True,\n",
        ")"
      ],
      "metadata": {
        "id": "2aeVNuJ3IORR"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test:"
      ],
      "metadata": {
        "id": "tCYjO5btILKW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampler_test = GFSLEvalEpisodeSampler(\n",
        "    cti_test_base, cti_test_novel, len(test_novel), n_tasks=TEST_TASKS, rng=test_rng,\n",
        "    n_way=N_WAY, k_shot=K_SHOT, q_novel=Q_NOVEL, q_base_total=Q_BASE_TOTAL,\n",
        ")"
      ],
      "metadata": {
        "id": "_5qt7i5sIppg"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a single test dataset by concatenating the two subsets.\n",
        "# Order matters: novel part first, base part second.\n",
        "test_concat = ConcatDataset([test_novel, test_base])"
      ],
      "metadata": {
        "id": "R0ZCnKHVfmo2"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader = DataLoader(\n",
        "    test_concat,\n",
        "    batch_sampler=sampler_test,   # emits indices into the single concatenated dataset\n",
        "    collate_fn=eval_collate_fn,   # reconstructs (support/query etc.) from that batch\n",
        "    num_workers=2,\n",
        "    pin_memory=True,\n",
        ")"
      ],
      "metadata": {
        "id": "l5nkZ9_uvDbw"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.1.2 DFSLwF module**\n",
        "\n",
        "???"
      ],
      "metadata": {
        "id": "Q4Zt-0FOrjzs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvBlock(nn.Module):\n",
        "    \"\"\"Conv3x3 -> BN -> ReLU -> (optional MaxPool2d).\"\"\"\n",
        "    def __init__(self, in_ch, out_ch, pool: bool):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn   = nn.BatchNorm2d(out_ch)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.pool = nn.MaxPool2d(2) if pool else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x); x = self.bn(x); x = self.relu(x); x = self.pool(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class FeatureExtractor(nn.Module):\n",
        "    \"\"\"Feature extractor with selectable backbone: 'conv' (light) or 'resnet18'.\n",
        "\n",
        "    Args:\n",
        "        backbone: 'conv' or 'resnet18'.\n",
        "        normalize_out: if True, L2-normalize the output features.\n",
        "        resnet_pretrained: if True (only for 'resnet18'), load ImageNet pretrained weights.\n",
        "        remove_last_relu: if True (only for 'resnet18'), remove the last post-add ReLU in the final BasicBlock\n",
        "                          (useful with cosine classifiers, per DFSLwF ablations).\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        backbone: str = \"conv\",\n",
        "        normalize_out: bool = True,\n",
        "        resnet_pretrained: bool = True,\n",
        "        remove_last_relu: bool = True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.normalize_out = normalize_out\n",
        "\n",
        "        if backbone.lower() == \"conv\":\n",
        "            # Lightweight CIFAR-style CNN trained from scratch: 64-D output\n",
        "            self.fe = nn.Sequential(\n",
        "                ConvBlock(3,   64, pool=True),   # 32 -> 16\n",
        "                ConvBlock(64,  64, pool=True),   # 16 -> 8\n",
        "                ConvBlock(64,  64, pool=False),\n",
        "                ConvBlock(64,  64, pool=False),\n",
        "                nn.AdaptiveAvgPool2d(1),         # -> (B,64,1,1)\n",
        "            )\n",
        "            self._mode = \"conv\"\n",
        "            self.out_dim = 64\n",
        "\n",
        "        elif backbone.lower() == \"resnet18\":\n",
        "            weights = ResNet18_Weights.IMAGENET1K_V1 if resnet_pretrained else None\n",
        "            m = resnet18(weights=weights)\n",
        "            m.fc = nn.Identity()  # we want the 512-D penultimate features\n",
        "\n",
        "            if remove_last_relu:\n",
        "                # Patch only the final BasicBlock to skip the post-add ReLU\n",
        "                last_block = m.layer4[-1]\n",
        "\n",
        "                def forward_norelu(self_block, x):\n",
        "                    identity = x\n",
        "                    out = self_block.conv1(x); out = self_block.bn1(out); out = self_block.relu(out)\n",
        "                    out = self_block.conv2(out); out = self_block.bn2(out)\n",
        "                    if self_block.downsample is not None:\n",
        "                        identity = self_block.downsample(x)\n",
        "                    out = out + identity\n",
        "                    return out  # no final ReLU\n",
        "\n",
        "                last_block.forward = types.MethodType(forward_norelu, last_block)\n",
        "\n",
        "            self.fe = m\n",
        "            self._mode = \"resnet18\"\n",
        "            self.out_dim = 512\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown backbone '{backbone}'. Use 'conv' or 'resnet18'.\")\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if self._mode == \"conv\":\n",
        "            z = self.fe(x).squeeze(-1).squeeze(-1)    # (B, 64)\n",
        "        else:  # resnet18\n",
        "            z = self.fe(x)                            # (B, 512)\n",
        "        return l2_normalize(z, dim=1) if self.normalize_out else z"
      ],
      "metadata": {
        "id": "BmbYd7mBr2y3"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CosineClassifier(nn.Module):\n",
        "    \"\"\"Cosine classifier with learnable temperature τ (tau).\"\"\"\n",
        "    def __init__(self, in_dim: int, n_classes: int, init_scale: float = TAU_INIT):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.empty(n_classes, in_dim))\n",
        "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
        "        self.tau = nn.Parameter(torch.tensor(float(init_scale)))\n",
        "\n",
        "    def forward(self, feats: torch.Tensor) -> torch.Tensor:\n",
        "        W = l2_normalize(self.weight, dim=1)\n",
        "        feats = l2_normalize(feats, dim=1)\n",
        "        logits = feats @ W.t()\n",
        "        return self.tau * logits"
      ],
      "metadata": {
        "id": "6DmrwTPYr3Xd"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FewShotWeightGenerator(nn.Module):\n",
        "    \"\"\"\n",
        "    [DFSLwF] Few-shot classification weight generator = Avg + Attention:\n",
        "      - w_avg  = mean(z_i)                     → φ_avg ⊙ w_avg\n",
        "      - w_att  = avg_i softmax(γ cos(φ_q z_i, k_b)) · w_b (over base classes b) → φ_att ⊙ w_att\n",
        "      - w'     = φ_avg ⊙ w_avg + φ_att ⊙ w_att, then L2-normalize\n",
        "    Includes:\n",
        "      - learnable keys k_b (one per base class), size (C_base, D)\n",
        "      - learnable φ_q (Linear D→D, no bias), φ_avg, φ_att (vectors), γ (scalar)\n",
        "      - optional dropout on features during training (Stage-2)\n",
        "    \"\"\"\n",
        "    def __init__(self, dim: int, num_base: int, p_dropout: float = 0.5):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.num_base = num_base\n",
        "        self.phi_q = nn.Linear(dim, dim, bias=False)\n",
        "        nn.init.kaiming_uniform_(self.phi_q.weight, a=math.sqrt(5))\n",
        "        self.keys = nn.Parameter(l2_normalize(torch.randn(num_base, dim), dim=1))\n",
        "        self.phi_avg = nn.Parameter(torch.ones(dim))\n",
        "        self.phi_att = nn.Parameter(torch.ones(dim))\n",
        "        self.gamma = nn.Parameter(torch.tensor(10.0))  # attention temperature (like τ)\n",
        "        self.dropout = nn.Dropout(p=p_dropout)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        support_feats: torch.Tensor,          # (N*K, D), L2-normalized\n",
        "        base_weights: torch.Tensor,           # (C_base, D), not necessarily normalized\n",
        "        shots_per_class: int,\n",
        "        exclude_mask: Optional[torch.Tensor] = None  # (C_base,) bool; True = keep; False = exclude\n",
        "    ) -> torch.Tensor:\n",
        "        D = support_feats.size(1)\n",
        "        N = support_feats.size(0) // shots_per_class\n",
        "        z = support_feats.view(N, shots_per_class, D)\n",
        "        if self.training:\n",
        "            z = self.dropout(z)\n",
        "        # w_avg\n",
        "        w_avg = l2_normalize(z.mean(dim=1), dim=1)  # (N, D)\n",
        "        # attention over base weights\n",
        "        Wb = l2_normalize(base_weights, dim=1)      # (C_base, D)\n",
        "        Kb = l2_normalize(self.keys, dim=1)         # (C_base, D)\n",
        "        # queries\n",
        "        q = self.phi_q(z)                           # (N, K, D)\n",
        "        q = l2_normalize(q, dim=2)                 # normalize across D\n",
        "        # cosine(q, Kb) => (N, K, C_base)\n",
        "        att_logits = torch.einsum(\"nkd,bd->nkb\", q, Kb) * self.gamma\n",
        "        if exclude_mask is not None:\n",
        "            # set -inf on excluded classes before softmax\n",
        "            mask = exclude_mask.view(1, 1, -1)  # broadcast\n",
        "            att_logits = att_logits.masked_fill(~mask, float(\"-inf\"))\n",
        "        att = torch.softmax(att_logits, dim=2)      # (N, K, C_base)\n",
        "        # weighted sum of base weights -> (N, K, D), then average over K (shots)\n",
        "        w_att = torch.einsum(\"nkb,bd->nkd\", att, Wb).mean(dim=1)  # (N, D)\n",
        "        # combine\n",
        "        w = self.phi_avg * w_avg + self.phi_att * w_att            # Hadamard\n",
        "        w = l2_normalize(w, dim=1)  # (N, D)\n",
        "        return w"
      ],
      "metadata": {
        "id": "1SVW654R6Fmp"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DFSLwF(nn.Module):\n",
        "    def __init__(self, fe: FeatureExtractor, clf_base: CosineClassifier, gen: FewShotWeightGenerator):\n",
        "        super().__init__()\n",
        "        self.fe = fe\n",
        "        self.clf_base = clf_base\n",
        "        self.gen = gen\n",
        "\n",
        "    def forward_logits(self, x: torch.Tensor, novel_weights: torch.Tensor | None = None) -> torch.Tensor:\n",
        "        feats = self.fe(x)\n",
        "        Wb = l2_normalize(self.clf_base.weight, dim=1)\n",
        "        logits = self.clf_base.tau * (feats @ Wb.t())\n",
        "        if novel_weights is not None and novel_weights.numel() > 0:\n",
        "            Wn = l2_normalize(novel_weights, dim=1)\n",
        "            logits_n = self.clf_base.tau * (feats @ Wn.t())\n",
        "            logits = torch.cat([logits, logits_n], dim=1)\n",
        "        return logits\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def build_novel_weights(self, support_imgs: torch.Tensor, k_shot: int) -> torch.Tensor:\n",
        "        supp = self.fe(support_imgs)  # (N*K, D), already normalized\n",
        "        Wb = self.clf_base.weight\n",
        "        return self.gen(supp, Wb, k_shot, exclude_mask=None)"
      ],
      "metadata": {
        "id": "Sf1nyTmgsJmb"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if BACKBONE == \"conv\":\n",
        "    fe = FeatureExtractor(backbone=\"conv\", normalize_out=True)\n",
        "else:\n",
        "    fe = FeatureExtractor(backbone=\"resnet18\", normalize_out=True, resnet_pretrained=True, remove_last_relu=True)\n",
        "\n",
        "clf = CosineClassifier(in_dim=fe.out_dim, n_classes=len(base_order))\n",
        "gen = FewShotWeightGenerator(dim=fe.out_dim, num_base=len(base_order), p_dropout=0.5)\n",
        "\n",
        "model = DFSLwF(fe=fe, clf_base=clf, gen=gen).to(device)"
      ],
      "metadata": {
        "id": "zXx49_x5vS-U",
        "outputId": "68c511b5-fd29-403b-b56d-d005e0fbc240",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 177MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_model = copy.deepcopy(model)"
      ],
      "metadata": {
        "id": "V6-K32td3uTN"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.2 Training**\n",
        "\n",
        "???"
      ],
      "metadata": {
        "id": "pwqrdqc9DitQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.2.1 Stage 1: supervised base training**\n",
        "\n",
        "???"
      ],
      "metadata": {
        "id": "9rRpFEbisZky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def evaluate_stage1_base_top1(model: DFSLwF, loader: DataLoader, device) -> float:\n",
        "    model.fe.eval(); model.clf_base.eval()\n",
        "    correct, total = 0, 0\n",
        "    for xb, yb in loader:\n",
        "        xb = xb.to(device); yb = yb.to(device)\n",
        "        logits = model.clf_base(model.fe(xb))\n",
        "        pred = logits.argmax(dim=1)\n",
        "        correct += (pred == yb).sum().item()\n",
        "        total   += yb.numel()\n",
        "    return correct / max(1, total)"
      ],
      "metadata": {
        "id": "0LvUzNjVF-6y"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_stage1(model: DFSLwF, loader: DataLoader, device: torch.device,\n",
        "                 epochs: int = STAGE1_EPOCHS, lr: float = STAGE1_LR,\n",
        "                 weight_decay: float = STAGE1_WEIGHT_DECAY,\n",
        "                 label_smoothing: float = STAGE1_LABEL_SMOOTH,\n",
        "                 val_loader: Optional[DataLoader] = None):\n",
        "\n",
        "    # [DFSLwF] Train feature extractor + base classifier (cosine)\n",
        "    model.fe.train(); model.clf_base.train(); model.gen.eval()\n",
        "\n",
        "    # Keep BN learnable here (paper trains a standard classifier in Stage-1)\n",
        "    params    = list(model.fe.parameters()) + list(model.clf_base.parameters())\n",
        "    opt       = torch.optim.Adam(params, lr=lr, weight_decay=weight_decay)\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
        "\n",
        "    # Params for validation\n",
        "    best_val = -1.0\n",
        "    patience = 0\n",
        "    best_state = None\n",
        "\n",
        "    with tqdm(range(epochs), desc=\"[Stage1] Supervised base training\") as epbar:\n",
        "        for ep in epbar:\n",
        "            model.fe.train(); model.clf_base.train()\n",
        "            batch_losses = []\n",
        "\n",
        "            for i, (xb, yb) in enumerate(loader):\n",
        "                xb = xb.to(device); yb = yb.to(device)\n",
        "                opt.zero_grad()\n",
        "                logits = model.clf_base(model.fe(xb))\n",
        "                loss   = criterion(logits, yb)\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "                batch_losses.append(float(loss.item()))\n",
        "\n",
        "                if (i + 1) % S1_LOG_EVERY == 0:\n",
        "                    epbar.set_postfix(loss=f\"{sliding_avg(batch_losses, S1_LOG_EVERY):.4f}\")\n",
        "\n",
        "            # ---- PERIODIC VALIDATION ----\n",
        "            if val_loader is not None and ((ep + 1) % S1_VAL_EVERY == 0):\n",
        "                val_acc = evaluate_stage1_base_top1(model, val_loader, device)\n",
        "                epbar.write(f\"\\t[stage1/val] epoch {ep+1:03d}: acc_base={100*val_acc:.2f}%\")\n",
        "\n",
        "                if val_acc > best_val + 1e-6:\n",
        "                    best_val   = val_acc\n",
        "                    patience   = 0\n",
        "                    best_state = {\n",
        "                        \"model\": copy.deepcopy(model.state_dict()),\n",
        "                        \"opt\":   copy.deepcopy(opt.state_dict()),\n",
        "                        \"epoch\": ep + 1,\n",
        "                        \"val\":   best_val,\n",
        "                    }\n",
        "                else:\n",
        "                    patience += 1\n",
        "                    if patience >= S1_PATIENCE:\n",
        "                        epbar.write(f\"\\t[stage1] Early stopping (no improvement for {S1_PATIENCE} validations).\")\n",
        "                        break\n",
        "\n",
        "    # Restore best result\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state[\"model\"])"
      ],
      "metadata": {
        "id": "UKTZWOz4V6wh"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_stage1(model, train_loader_s1, device=device, val_loader=val_loader_s1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boHxe6JoviaS",
        "outputId": "a9a0389e-4322-44ee-870a-36385a58ca10"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage1] Supervised base training:   2%|▏         | 2/120 [00:56<56:04, 28.52s/it, loss=1.8609]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage1/val] epoch 002: acc_base=54.12%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage1] Supervised base training:   3%|▎         | 4/120 [01:53<55:32, 28.73s/it, loss=1.7185]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage1/val] epoch 004: acc_base=55.66%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage1] Supervised base training:   5%|▌         | 6/120 [02:50<54:28, 28.67s/it, loss=1.6462]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage1/val] epoch 006: acc_base=59.69%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage1] Supervised base training:   7%|▋         | 8/120 [03:46<53:17, 28.55s/it, loss=1.5909]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage1/val] epoch 008: acc_base=57.31%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage1] Supervised base training:   8%|▊         | 10/120 [04:42<51:49, 28.27s/it, loss=1.5528]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage1/val] epoch 010: acc_base=59.53%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage1] Supervised base training:  10%|█         | 12/120 [05:39<51:13, 28.45s/it, loss=1.5046]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage1/val] epoch 012: acc_base=60.75%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage1] Supervised base training:  12%|█▏        | 14/120 [06:35<50:16, 28.46s/it, loss=1.5097]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage1/val] epoch 014: acc_base=56.06%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage1] Supervised base training:  13%|█▎        | 16/120 [07:32<49:27, 28.53s/it, loss=1.4720]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage1/val] epoch 016: acc_base=60.53%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage1] Supervised base training:  15%|█▌        | 18/120 [08:28<48:21, 28.45s/it, loss=1.4453]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage1/val] epoch 018: acc_base=61.41%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage1] Supervised base training:  17%|█▋        | 20/120 [09:25<47:43, 28.63s/it, loss=1.4477]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage1/val] epoch 020: acc_base=64.78%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage1] Supervised base training:  18%|█▊        | 22/120 [10:21<46:32, 28.49s/it, loss=1.4067]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage1/val] epoch 022: acc_base=62.78%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage1] Supervised base training:  20%|██        | 24/120 [11:17<45:29, 28.43s/it, loss=1.4017]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage1/val] epoch 024: acc_base=59.34%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage1] Supervised base training:  22%|██▏       | 26/120 [12:15<44:59, 28.72s/it, loss=1.3856]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage1/val] epoch 026: acc_base=64.22%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage1] Supervised base training:  23%|██▎       | 28/120 [13:11<43:47, 28.56s/it, loss=1.3627]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage1/val] epoch 028: acc_base=61.75%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage1] Supervised base training:  24%|██▍       | 29/120 [14:08<44:24, 29.28s/it, loss=1.3489]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage1/val] epoch 030: acc_base=63.72%\n",
            "\t[stage1] Early stopping (no improvement for 5 validations).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stage1_model = copy.deepcopy(model)"
      ],
      "metadata": {
        "id": "MTUp7LJb31fW"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.2.2 Stage 2: episodic training**\n",
        "\n",
        "???"
      ],
      "metadata": {
        "id": "zYaBPBI4ssZq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def evaluate_gfsl(model: DFSLwF, test_loader: DataLoader, cifar_targets_all: List[int],\n",
        "                  base_classes: List[int], device: torch.device) -> Tuple[float, float, float]:\n",
        "    model.fe.eval(); model.clf_base.eval(); model.gen.eval()\n",
        "    acc_per_episode_base, acc_per_episode_novel = [], []\n",
        "    base_order = sorted(base_classes)\n",
        "    b2local = {cid: i for i, cid in enumerate(base_order)}\n",
        "    Cb = model.clf_base.weight.size(0)\n",
        "\n",
        "    for (support_novel, query_images, true_novel_ids, gt_novel, gt_base) in test_loader:\n",
        "        support_novel = support_novel.to(device)\n",
        "        query_images = query_images.to(device)\n",
        "        gt_novel = gt_novel.to(device)\n",
        "        gt_base = gt_base.to(device)\n",
        "\n",
        "        novel_weights = model.build_novel_weights(support_novel, K_SHOT)  # (N, D)\n",
        "        logits = model.forward_logits(query_images, novel_weights)\n",
        "        preds = logits.argmax(dim=1)\n",
        "\n",
        "        N = novel_weights.size(0)\n",
        "        pred_novel = preds[:N * Q_NOVEL] - Cb\n",
        "        pred_base = preds[N * Q_NOVEL:]\n",
        "\n",
        "        id2local = {cid: i for i, cid in enumerate(true_novel_ids)}\n",
        "        gt_novel_local = torch.tensor([id2local[int(y.item())] for y in gt_novel], device=device)\n",
        "        gt_base_local = torch.tensor([b2local[int(y.item())] for y in gt_base], device=device)\n",
        "\n",
        "        acc_b = (pred_base == gt_base_local).float().mean().item()\n",
        "        acc_n = (pred_novel == gt_novel_local).float().mean().item()\n",
        "        acc_per_episode_base.append(acc_b); acc_per_episode_novel.append(acc_n)\n",
        "\n",
        "    return len(acc_per_episode_base), gfsl_stats(acc_per_episode_base, acc_per_episode_novel)"
      ],
      "metadata": {
        "id": "Q0WCFplb3CNH"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_stage2(model: DFSLwF, meta_loader: DataLoader, device: torch.device,\n",
        "                 base_order: List[int], n_tasks: int = STAGE2_TASKS, val_every: int = STAGE2_VAL_EVERY,\n",
        "                 lr: float = STAGE2_LR, val_loader: Optional[DataLoader] = None):\n",
        "    \"\"\"\n",
        "    [DFSLwF] Freeze F, train generator + continue training W_base (and τ). Exclude pseudo-novel from attention memory.\n",
        "    Use *true* base labels for base queries; novel queries target indices are (Cb .. Cb+N-1).\n",
        "    \"\"\"\n",
        "    # Freeze feature extractor; freeze BN stats\n",
        "    model.fe.eval(); model.fe.apply(set_bn_eval)\n",
        "    for p in model.fe.parameters(): p.requires_grad = False\n",
        "\n",
        "    # Train generator, base weights, and tau\n",
        "    for p in model.clf_base.parameters(): p.requires_grad = True\n",
        "    params = list(model.gen.parameters()) + [model.clf_base.tau, model.clf_base.weight]\n",
        "    opt = torch.optim.Adam(params, lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # mapping CIFAR id -> local base index\n",
        "    b2local = {cid: i for i, cid in enumerate(sorted(base_order))}\n",
        "    Cb = model.clf_base.weight.size(0)\n",
        "\n",
        "    # Validation params\n",
        "    best_score = -1.0\n",
        "    patience   = 0\n",
        "    best_state = None\n",
        "\n",
        "    model.gen.train(); model.clf_base.train()\n",
        "    with tqdm(enumerate(meta_loader), total=len(meta_loader), desc=\"[Stage2] Episodic Training\") as pbar:\n",
        "        for step, (support_novel, query_images, true_novel_ids, base_q_labels_cifar) in pbar:\n",
        "            support_novel       = support_novel.to(device)\n",
        "            query_images        = query_images.to(device)\n",
        "            base_q_labels_cifar = base_q_labels_cifar.to(device)\n",
        "\n",
        "            # Mask: escludi pseudo-novel dall'attenzione\n",
        "            exclude_mask = torch.ones(Cb, dtype=torch.bool, device=device)\n",
        "            for cid in true_novel_ids:\n",
        "                if cid in b2local:\n",
        "                    exclude_mask[b2local[cid]] = False\n",
        "\n",
        "            # Novel weights (support -> gen), FE è congelato\n",
        "            with torch.no_grad():\n",
        "                supp_feats = model.fe(support_novel)\n",
        "            novel_weights = model.gen(supp_feats, model.clf_base.weight, K_SHOT, exclude_mask=exclude_mask)\n",
        "\n",
        "            # Logits e target\n",
        "            logits = model.forward_logits(query_images, novel_weights)  # [Cb | N]\n",
        "            N = novel_weights.size(0)\n",
        "\n",
        "            y_novel = torch.arange(N, device=device).repeat_interleave(Q_NOVEL)\n",
        "            targets = torch.empty(logits.size(0), dtype=torch.long, device=device)\n",
        "            targets[:N * Q_NOVEL] = Cb + y_novel\n",
        "            base_local = torch.tensor([b2local[int(y.item())] for y in base_q_labels_cifar], device=device)\n",
        "            targets[N * Q_NOVEL:] = base_local\n",
        "\n",
        "            opt.zero_grad()\n",
        "            loss = criterion(logits, targets)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(params, STAGE2_GRAD_CLIP)\n",
        "            opt.step()\n",
        "            pbar.set_postfix(loss=f\"{float(loss.item()):.4f}\")\n",
        "\n",
        "            # ---- PERIODIC VALIDATION ----\n",
        "            if val_loader is not None and ((step + 1) % val_every == 0):\n",
        "                Tval, vstats = evaluate_gfsl(model, val_loader, ds_train.targets, base_order, device)\n",
        "                v_base  = vstats[\"base\"][\"mean\"];  v_base_ci  = vstats[\"base\"][\"conf\"]\n",
        "                v_novel = vstats[\"novel\"][\"mean\"]; v_novel_ci = vstats[\"novel\"][\"conf\"]\n",
        "                v_h     = vstats[\"hmean\"][\"mean\"]; v_h_ci     = vstats[\"hmean\"][\"conf\"]\n",
        "\n",
        "                pbar.write(f\"\\t[stage2/val] step {step+1:05d}: \"\n",
        "                          f\"base={100*v_base:.2f}%±{100*v_base_ci:.2f}  \"\n",
        "                          f\"novel={100*v_novel:.2f}%±{100*v_novel_ci:.2f}  \"\n",
        "                          f\"h-mean={100*v_h:.2f}%±{100*v_h_ci:.2f}  (T={Tval})\")\n",
        "\n",
        "                sel = {\"base\": v_base, \"novel\": v_novel, \"hmean\": v_h}[S2_SELECT_METRIC]\n",
        "                if sel > best_score + 1e-6:\n",
        "                    best_score = sel\n",
        "                    patience   = 0\n",
        "                    best_state = {\n",
        "                        \"model\": copy.deepcopy(model.state_dict()),\n",
        "                        \"opt\":   copy.deepcopy(opt.state_dict()),\n",
        "                        \"step\":  step + 1,\n",
        "                        \"score\": best_score,\n",
        "                    }\n",
        "                else:\n",
        "                    patience += 1\n",
        "                    if patience >= S2_PATIENCE:\n",
        "                        pbar.write(f\"\\t[stage2] Early stopping (no improvement for {S2_PATIENCE} validations).\")\n",
        "                        break\n",
        "\n",
        "                model.gen.train(); model.clf_base.train()\n",
        "\n",
        "    # Restore the best result\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state[\"model\"])"
      ],
      "metadata": {
        "id": "PmIOgTtNsmZF"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_stage2(model, train_loader_s2, device=device, base_order=base_order, val_loader=val_loader_s2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffRMFtefvnma",
        "outputId": "7aaeda4e-1f31-45b7-9ddd-7501bb786761"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage2] Episodic Training:   3%|▎         | 502/20000 [03:50<102:27:11, 18.92s/it, loss=0.9040]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage2/val] step 00500: base=59.40%±0.36  novel=54.31%±0.56  h-mean=56.15%±0.34  (T=1000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage2] Episodic Training:   5%|▌         | 1002/20000 [07:39<120:45:57, 22.88s/it, loss=0.9245]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage2/val] step 01000: base=58.27%±0.36  novel=57.39%±0.58  h-mean=57.23%±0.34  (T=1000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage2] Episodic Training:   8%|▊         | 1502/20000 [11:29<92:33:29, 18.01s/it, loss=0.8914] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage2/val] step 01500: base=57.62%±0.36  novel=58.92%±0.58  h-mean=57.68%±0.32  (T=1000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage2] Episodic Training:  10%|█         | 2002/20000 [15:17<102:34:15, 20.52s/it, loss=0.8622]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage2/val] step 02000: base=57.05%±0.35  novel=60.72%±0.56  h-mean=58.31%±0.32  (T=1000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage2] Episodic Training:  13%|█▎        | 2502/20000 [19:07<99:15:21, 20.42s/it, loss=0.7475] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage2/val] step 02500: base=56.53%±0.36  novel=61.56%±0.58  h-mean=58.37%±0.32  (T=1000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage2] Episodic Training:  15%|█▌        | 3002/20000 [22:56<85:16:59, 18.06s/it, loss=0.8587] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage2/val] step 03000: base=56.44%±0.36  novel=61.61%±0.58  h-mean=58.35%±0.33  (T=1000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage2] Episodic Training:  18%|█▊        | 3502/20000 [26:45<77:53:25, 17.00s/it, loss=0.7389] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage2/val] step 03500: base=56.05%±0.36  novel=62.38%±0.57  h-mean=58.53%±0.31  (T=1000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage2] Episodic Training:  20%|██        | 4002/20000 [30:33<92:22:34, 20.79s/it, loss=0.5754] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage2/val] step 04000: base=55.91%±0.37  novel=62.58%±0.56  h-mean=58.53%±0.31  (T=1000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage2] Episodic Training:  23%|██▎       | 4502/20000 [34:22<89:02:44, 20.68s/it, loss=0.6477] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage2/val] step 04500: base=56.14%±0.38  novel=62.11%±0.57  h-mean=58.40%±0.32  (T=1000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage2] Episodic Training:  25%|██▌       | 5002/20000 [38:12<67:21:42, 16.17s/it, loss=0.9096]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage2/val] step 05000: base=56.09%±0.37  novel=62.10%±0.56  h-mean=58.39%±0.31  (T=1000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage2] Episodic Training:  28%|██▊       | 5502/20000 [42:00<80:16:20, 19.93s/it, loss=0.6911] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage2/val] step 05500: base=56.10%±0.35  novel=62.98%±0.58  h-mean=58.82%±0.32  (T=1000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage2] Episodic Training:  30%|███       | 6002/20000 [45:48<64:52:21, 16.68s/it, loss=0.7718]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage2/val] step 06000: base=56.44%±0.37  novel=63.58%±0.57  h-mean=59.25%±0.32  (T=1000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage2] Episodic Training:  33%|███▎      | 6502/20000 [49:40<67:52:12, 18.10s/it, loss=0.7181] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage2/val] step 06500: base=55.87%±0.38  novel=63.51%±0.58  h-mean=58.88%±0.32  (T=1000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage2] Episodic Training:  35%|███▌      | 7002/20000 [53:27<76:49:56, 21.28s/it, loss=0.6225] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage2/val] step 07000: base=55.80%±0.37  novel=63.78%±0.57  h-mean=59.00%±0.32  (T=1000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage2] Episodic Training:  38%|███▊      | 7502/20000 [57:15<54:44:03, 15.77s/it, loss=0.6612]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage2/val] step 07500: base=55.93%±0.36  novel=63.63%±0.56  h-mean=59.01%±0.31  (T=1000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage2] Episodic Training:  40%|████      | 8002/20000 [1:01:00<58:03:40, 17.42s/it, loss=0.7231]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage2/val] step 08000: base=55.72%±0.36  novel=63.61%±0.56  h-mean=58.89%±0.31  (T=1000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage2] Episodic Training:  42%|████▏     | 8499/20000 [1:04:55<1:27:52,  2.18it/s, loss=0.7663]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t[stage2/val] step 08500: base=56.15%±0.36  novel=63.60%±0.56  h-mean=59.15%±0.31  (T=1000)\n",
            "\t[stage2] Early stopping (no improvement for 5 validations).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.3 Evaluation**\n",
        "\n",
        "???"
      ],
      "metadata": {
        "id": "DfsB1tdtsyKO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "T, stats = evaluate_gfsl(model, test_loader, ds_test.targets, base, device=device)\n",
        "print_stats(T, stats, model=\"After stage 2 training\")"
      ],
      "metadata": {
        "id": "71N0971Mvqm6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "817edc28-957c-4cc1-d32c-26998eb3b48f"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[test] - After stage 2 training (95% CI on 1000 tasks)\n",
            " - [Base]   acc=57.19% ± 0.36%\n",
            " - [Novel]  acc=62.14% ± 0.60%\n",
            " - [H-mean] acc=58.98% ± 0.33%\n"
          ]
        }
      ]
    }
  ]
}