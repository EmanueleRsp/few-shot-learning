{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Generalized Few-Shot Learning (GFSL)**\n",
        "\n",
        "Intro"
      ],
      "metadata": {
        "id": "tFoEk1MspnUb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "PgPkCPJxpl63"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Background**"
      ],
      "metadata": {
        "id": "CFGadMzopx5K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.1 Generalized Few-Shot Leaning (GFSL)**\n",
        "\n",
        "???"
      ],
      "metadata": {
        "id": "tiLuW00wp55a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.2 Dynamic Few-Shot Learning without Forgetting**\n",
        "\n",
        "???"
      ],
      "metadata": {
        "id": "ZJGmdC0np5IB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.3 CIFAR100 dataset**\n",
        "\n",
        "???"
      ],
      "metadata": {
        "id": "5CzeVUAMqNiN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "AXRqQlJspyNl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Practice**"
      ],
      "metadata": {
        "id": "UAwYkSFHoebf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "import math\n",
        "import random\n",
        "import types\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "import copy\n",
        "from functools import partial\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import ConcatDataset, DataLoader, Subset\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import CIFAR100\n",
        "from torchvision.models import resnet18, ResNet18_Weights\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "0c0fLQwq5t_4"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seed:"
      ],
      "metadata": {
        "id": "nlrbjXRRoo-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 42\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "train_rng = np.random.default_rng(SEED + 1)\n",
        "test_rng = np.random.default_rng(SEED + 2)"
      ],
      "metadata": {
        "id": "h0QZmdPV52XF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Settings:"
      ],
      "metadata": {
        "id": "_6NNqLzFoxUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Few-shot / episodic config (Stage 2 + Test)\n",
        "N_WAY = 5\n",
        "K_SHOT = 5\n",
        "Q_NOVEL = 15\n",
        "Q_BASE_TOTAL = 75\n",
        "\n",
        "# CIFAR-100 split sizes\n",
        "N_BASE = 64\n",
        "N_VALNOVEL = 16\n",
        "N_TESTNOVEL = 20\n",
        "\n",
        "# Initial network params\n",
        "TAU_INIT = 10.0  # temperature init\n",
        "\n",
        "# Stage 1\n",
        "STAGE1_EPOCHS = 120\n",
        "STAGE1_LR = 3e-3\n",
        "STAGE1_BS = 512\n",
        "STAGE1_WEIGHT_DECAY = 5e-4\n",
        "STAGE1_LABEL_SMOOTH = 0.1\n",
        "\n",
        "# Stage 2\n",
        "STAGE2_TASKS = 20_000\n",
        "STAGE2_VAL_EVERY = 500\n",
        "STAGE2_LR = 5e-4\n",
        "STAGE2_GRAD_CLIP = 1.0\n",
        "\n",
        "# Test\n",
        "TEST_TASKS = 1_000"
      ],
      "metadata": {
        "id": "MvrrIQjKoxLe"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utility functions:"
      ],
      "metadata": {
        "id": "NOTdCwcdpGaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_cifar100_classes(seed: int, n_base=64, n_val=16, n_test=20):\n",
        "    \"\"\"Split CIFAR-100 class IDs into base/val-novel/test-novel sets.\n",
        "\n",
        "    Shuffles the 100 class IDs with a reproducible RNG and returns three\n",
        "    disjoint lists for base classes (used for supervised training), validation\n",
        "    novel classes (optional episodic validation), and test novel classes\n",
        "    (used in GFSL evaluation).\n",
        "\n",
        "    Args:\n",
        "        seed: Random seed for the class shuffling.\n",
        "        n_base: Number of base classes.\n",
        "        n_val: Number of validation novel classes.\n",
        "        n_test: Number of test novel classes.\n",
        "\n",
        "    Returns:\n",
        "        A tuple (base, valn, testn) where each element is a list of class IDs.\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "    classes = np.arange(100); rng.shuffle(classes)\n",
        "    return classes[:n_base].tolist(), classes[n_base:n_base+n_val].tolist(), classes[n_base+n_val:n_base+n_val+n_test].tolist()\n",
        "\n",
        "def subset_by_classes(ds, keep):\n",
        "    \"\"\"Return a Subset containing only samples whose label is in `keep`.\n",
        "\n",
        "    Uses vectorized filtering over `ds.targets` to select the indices that\n",
        "    belong to the provided set of class IDs.\n",
        "\n",
        "    Args:\n",
        "        ds: A torchvision-style dataset exposing `targets` (list/array of ints).\n",
        "        keep: Iterable of class IDs to retain.\n",
        "\n",
        "    Returns:\n",
        "        torch.utils.data.Subset wrapping `ds` with filtered indices.\n",
        "    \"\"\"\n",
        "    t = np.array(ds.targets)\n",
        "    idx = np.nonzero(np.isin(t, keep))[0]\n",
        "    return Subset(ds, idx)\n",
        "\n",
        "def class_to_local_indices(subset):\n",
        "    \"\"\"Build a mapping class_id -> list of *local* indices within `subset`.\n",
        "\n",
        "    Iterates over the subset indices and groups them by their original class\n",
        "    ID (read from `subset.dataset.targets`). Useful for fast episodic sampling\n",
        "    (e.g., drawing K support and Q query images per class).\n",
        "\n",
        "    Args:\n",
        "        subset: A torch Subset whose `dataset` exposes `targets`\n",
        "            and whose `indices` reference the original dataset.\n",
        "\n",
        "    Returns:\n",
        "        Dict[int, List[int]] mapping each class ID to a list of local indices\n",
        "        (0..len(subset)-1) within the subset.\n",
        "\n",
        "    Notes:\n",
        "        The returned indices are local to `subset` (not the original dataset).\n",
        "    \"\"\"\n",
        "    t = np.array(subset.dataset.targets)\n",
        "    out = {}\n",
        "    for j, i in enumerate(subset.indices):\n",
        "        y = int(t[i])\n",
        "        (out.setdefault(y, [])).append(j)\n",
        "    return out"
      ],
      "metadata": {
        "id": "bRJokFSoC3Wo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def l2_normalize(x: torch.Tensor, dim: int = 1, eps: float = 1e-6) -> torch.Tensor:\n",
        "  \"\"\"L2-normalize a tensor along a given dimension.\n",
        "\n",
        "  Each vector along `dim` is divided by its L2 norm, producing unit-length\n",
        "  vectors. A small epsilon is used to avoid division by zero.\n",
        "\n",
        "  Args:\n",
        "      x: Input tensor.\n",
        "      dim: Dimension along which to compute the L2 norm (default: 1).\n",
        "      eps: Minimum norm value used for numerical stability (default: 1e-6).\n",
        "\n",
        "  Returns:\n",
        "      A tensor with the same shape as `x`, L2-normalized along `dim`.\n",
        "  \"\"\"\n",
        "  return x / (x.norm(p=2, dim=dim, keepdim=True).clamp_min(eps))"
      ],
      "metadata": {
        "id": "g6FYkq_npGJR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_bn_eval(m: nn.Module):\n",
        "  \"\"\"Put BatchNorm2d layers in eval mode and freeze their parameters.\n",
        "\n",
        "  When applied (e.g., `model.apply(set_bn_eval)`), this sets each\n",
        "  `nn.BatchNorm2d` module to evaluation mode so it uses stored running\n",
        "  statistics and stops updating them, and it disables gradient updates\n",
        "  for its affine parameters (gamma/beta).\n",
        "\n",
        "  Args:\n",
        "      m: A module that may be an instance of `nn.BatchNorm2d`.\n",
        "\n",
        "  Returns:\n",
        "      None. The module is modified in place if it is BatchNorm2d.\n",
        "  \"\"\"\n",
        "  if isinstance(m, nn.BatchNorm2d):\n",
        "      m.eval()\n",
        "      for p in m.parameters():\n",
        "          p.requires_grad = False"
      ],
      "metadata": {
        "id": "nh0Z36WvpJ1j"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sliding_avg(xs: List[float], k: int = 20) -> float:\n",
        "  if not xs:\n",
        "      return 0.0\n",
        "  if len(xs) < k:\n",
        "      return float(sum(xs) / len(xs))\n",
        "  return float(sum(xs[-k:]) / k)"
      ],
      "metadata": {
        "id": "aXkLYzad5_UO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_ci95(xs: np.ndarray) -> Tuple[float, float]:\n",
        "    xs = np.asarray(xs, dtype=float)\n",
        "    n  = xs.size\n",
        "    if n < 2:\n",
        "        return float(xs.mean()), 0.0\n",
        "    std = xs.std(ddof=1)\n",
        "    stderr = std / np.sqrt(n)\n",
        "    z = 1.96\n",
        "    return float(xs.mean()), float(z * stderr)\n",
        "\n",
        "def gfsl_stats(\n",
        "    acc_per_ep_base: List[float],\n",
        "    acc_per_ep_novel: List[float],\n",
        ") -> Dict[str, Dict[str, float]]:\n",
        "\n",
        "    if len(acc_per_ep_base) != len(acc_per_ep_novel):\n",
        "        raise ValueError(\"base and novel must be of the same length\")\n",
        "    T = len(acc_per_ep_base)\n",
        "\n",
        "    base = np.asarray(acc_per_ep_base, dtype=float)\n",
        "    novel = np.asarray(acc_per_ep_novel, dtype=float)\n",
        "\n",
        "    denom = base + novel\n",
        "    h_per_ep = np.where(denom > 0, 2.0 * base * novel / denom, 0.0)\n",
        "\n",
        "    base_mean, base_ci  = mean_ci95(base)\n",
        "    novel_mean, novel_ci = mean_ci95(novel)\n",
        "    h_mean, h_ci = mean_ci95(h_per_ep)\n",
        "\n",
        "    return {\n",
        "        \"base\":  {\"mean\": base_mean,  \"conf\": base_ci},\n",
        "        \"novel\": {\"mean\": novel_mean, \"conf\": novel_ci},\n",
        "        \"hmean\": {\"mean\": h_mean,     \"conf\": h_ci},\n",
        "    }\n",
        "\n",
        "def print_stats(T: int, stats: Dict[str, Dict[str, float]], model: str = \"\"):\n",
        "  print(f\"[test] - {model} (95% CI on {T} tasks)\")\n",
        "  print(f\" - [Base]   acc={100*stats['base']['mean']:.2f}% ± {100*stats['base']['conf']:.2f}%\")\n",
        "  print(f\" - [Novel]  acc={100*stats['novel']['mean']:.2f}% ± {100*stats['novel']['conf']:.2f}%\")\n",
        "  print(f\" - [H-mean] acc={100*stats['hmean']['mean']:.2f}% ± {100*stats['hmean']['conf']:.2f}%\")\n"
      ],
      "metadata": {
        "id": "RWvm92IonBf3"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.1 Environment**"
      ],
      "metadata": {
        "id": "aRvCumaepV4w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.1.1 CIFAR100 dataset**\n",
        "\n",
        "???"
      ],
      "metadata": {
        "id": "NMpMP2wfpYWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transforms\n",
        "IMAGE_SIZE = 32\n",
        "\n",
        "train_tf = transforms.Compose([\n",
        "    transforms.RandomCrop(IMAGE_SIZE, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "eval_tf = transforms.Compose([transforms.ToTensor(),])"
      ],
      "metadata": {
        "id": "HwaG9qKI57p6"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_train = CIFAR100(root=\"./data\", train=True,  transform=train_tf, download=True)\n",
        "ds_test  = CIFAR100(root=\"./data\", train=False, transform=eval_tf,  download=True)\n",
        "\n",
        "base, valn, testn = split_cifar100_classes(SEED)\n",
        "\n",
        "train_base = subset_by_classes(ds_train, base)\n",
        "test_base  = subset_by_classes(ds_test,  base)\n",
        "test_novel = subset_by_classes(ds_test,  testn)\n",
        "\n",
        "cti_train_base = class_to_local_indices(train_base)\n",
        "cti_test_base  = class_to_local_indices(test_base)\n",
        "cti_test_novel = class_to_local_indices(test_novel)"
      ],
      "metadata": {
        "id": "QcTRsE73tYyU"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.1.1.1 DataLoader: Stage 1**"
      ],
      "metadata": {
        "id": "m0uFfFH8tbjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Stage1TrainDS(torch.utils.data.Dataset):\n",
        "    def __init__(self, subset: Subset, orig_targets: List[int], order_map: Dict[int, int]):\n",
        "        \"\"\"Subset over base classes con label locali precompute [0..Cb-1].\"\"\"\n",
        "        self.subset = subset\n",
        "        self.local_labels = [order_map[int(orig_targets[i])] for i in subset.indices]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.subset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x, _ = self.subset[idx]\n",
        "        return x, self.local_labels[idx]"
      ],
      "metadata": {
        "id": "bIui-IjnGOfr"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_order = sorted(base)\n",
        "order_map = {cid: i for i, cid in enumerate(base_order)}\n",
        "stage1_train_ds = Stage1TrainDS(train_base, ds_train.targets, order_map)\n",
        "\n",
        "train_loader_s1 = DataLoader(\n",
        "    stage1_train_ds,\n",
        "    batch_size=STAGE1_BS,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True,\n",
        ")"
      ],
      "metadata": {
        "id": "7UGqx2oStiVD"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.1.1.2 DataLoader: Stage 2**"
      ],
      "metadata": {
        "id": "yGENDGSFtivp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GFSLTrainEpisodicBatchSampler:\n",
        "    \"\"\"\n",
        "    Stage-2: N_WAY pseudo-novel (from BASE train) with K+Qn each + Qb base queries from BASE (any class).\n",
        "    Returns indices over the Subset(train_base).\n",
        "    \"\"\"\n",
        "    def __init__(self, class_to_indices: Dict[int, List[int]], n_tasks: int, rng: np.random.Generator,\n",
        "                 n_way: int = N_WAY, k_shot: int = K_SHOT, q_novel: int = Q_NOVEL, q_base_total: int = Q_BASE_TOTAL):\n",
        "        self.cti = class_to_indices\n",
        "        self.all_classes = list(class_to_indices.keys())\n",
        "        self.n_tasks = n_tasks\n",
        "        self.rng = rng\n",
        "        self.n_way = n_way\n",
        "        self.k_shot = k_shot\n",
        "        self.q_novel = q_novel\n",
        "        self.q_base_total = q_base_total\n",
        "\n",
        "    def __len__(self): return self.n_tasks\n",
        "\n",
        "    def __iter__(self):\n",
        "        for _ in range(self.n_tasks):\n",
        "\n",
        "            novel = self.rng.choice(self.all_classes, size=self.n_way, replace=False)\n",
        "\n",
        "            batch = []\n",
        "            for c in novel:\n",
        "                pool = self.cti[c]\n",
        "                need = self.k_shot + self.q_novel\n",
        "                if len(pool) < need:\n",
        "                    raise ValueError(f\"Class {c} has {len(pool)} < {need}\")\n",
        "                idx = self.rng.choice(pool, size=need, replace=False)\n",
        "                batch.append(idx)\n",
        "\n",
        "            novel_classes = set(novel.tolist())\n",
        "            base_pool_classes = [c for c in self.all_classes if c not in novel_classes]\n",
        "\n",
        "            used = set(np.concatenate(batch).tolist())\n",
        "\n",
        "            base_q = []\n",
        "            while len(base_q) < self.q_base_total:\n",
        "                c = int(self.rng.choice(base_pool_classes))\n",
        "                cand = int(self.rng.choice(self.cti[c]))\n",
        "                if cand not in used:\n",
        "                    base_q.append(cand)\n",
        "                    used.add(cand)\n",
        "\n",
        "            batch.append(np.array(base_q, dtype=int))\n",
        "            yield np.concatenate(batch)\n",
        "\n",
        "stage2_train_batch_sampler = GFSLTrainEpisodicBatchSampler(\n",
        "    cti_train_base,\n",
        "    n_tasks=STAGE2_TASKS,\n",
        "    rng=train_rng,\n",
        "    n_way=N_WAY, k_shot=K_SHOT, q_novel=Q_NOVEL, q_base_total=Q_BASE_TOTAL\n",
        ")"
      ],
      "metadata": {
        "id": "jWdAwrxL6KH-"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stage2_collate(batch, n_way: int, k_shot: int, q_novel: int, q_base_total: int):\n",
        "    \"\"\"\n",
        "    Output:\n",
        "      - support_novel: (N*K, C, H, W)\n",
        "      - query_images : (N*Qn + Qb, C, H, W)\n",
        "      - true_novel_ids (original CIFAR ids, one per novel class)\n",
        "      - base_query_labels_cifar: (Qb,) original CIFAR ids for base queries (for true targets)\n",
        "    \"\"\"\n",
        "    imgs, labs = list(zip(*batch))\n",
        "    images = torch.stack(imgs)\n",
        "    labels = torch.tensor([int(y) for y in labs])\n",
        "\n",
        "    per_novel = k_shot + q_novel\n",
        "    total_novel = n_way * per_novel\n",
        "\n",
        "    novel_block = images[:total_novel].view(n_way, per_novel, *images.shape[1:])\n",
        "    novel_labels_block = labels[:total_novel].view(n_way, per_novel)\n",
        "\n",
        "    support_novel = novel_block[:, :k_shot].reshape(-1, *images.shape[1:])\n",
        "    query_novel = novel_block[:, k_shot:].reshape(-1, *images.shape[1:])\n",
        "    query_base = images[total_novel:]\n",
        "\n",
        "    query_images = torch.cat([query_novel, query_base], dim=0)\n",
        "\n",
        "    true_novel_ids = [int(novel_labels_block[i, 0].item()) for i in range(n_way)]\n",
        "    base_query_labels_cifar = labels[total_novel:]  # original CIFAR ids\n",
        "    return support_novel, query_images, true_novel_ids, base_query_labels_cifar\n",
        "\n",
        "stage2_collate_fn = partial(\n",
        "    stage2_collate,\n",
        "    n_way=N_WAY, k_shot=K_SHOT, q_novel=Q_NOVEL, q_base_total=Q_BASE_TOTAL,\n",
        ")"
      ],
      "metadata": {
        "id": "VKDRe5296RFa"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Stage2TrainDS(torch.utils.data.Dataset):\n",
        "    def __init__(self, subset: Subset):\n",
        "        self.subset = subset\n",
        "    def __len__(self):\n",
        "      return len(self.subset)\n",
        "    def __getitem__(self, idx):\n",
        "      return self.subset[idx]\n",
        "\n",
        "stage2_train_ds = Stage2TrainDS(train_base)"
      ],
      "metadata": {
        "id": "zyl7POdxWpoE"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader_s2 = DataLoader(\n",
        "    stage2_train_ds,\n",
        "    batch_sampler=stage2_train_batch_sampler,\n",
        "    collate_fn=stage2_collate_fn,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")"
      ],
      "metadata": {
        "id": "0-sHBaXruHqM"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.1.1.3 DataLoader: Test**"
      ],
      "metadata": {
        "id": "mxGopBG5u_6A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GFSLEvalEpisodeSampler:\n",
        "    \"\"\"Batch sampler for GFSL test episodes over a single ConcatDataset.\n",
        "\n",
        "    Each yielded batch is a 1D numpy array of indices into `test_concat`\n",
        "    laid out as:\n",
        "        [ N_WAY*(K_SHOT+Q_NOVEL) indices from novel part | Q_BASE_TOTAL indices from base part ]\n",
        "\n",
        "    This mirrors Stage-2 structure, but draws from the *test* splits:\n",
        "      - support + query for true novel classes from the novel test subset\n",
        "      - base queries from the base test subset\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, base_cti: Dict[int, List[int]], novel_cti: Dict[int, List[int]], offset_base: int, n_tasks: int, rng: np.random.Generator,\n",
        "                 n_way: int = N_WAY, k_shot: int = K_SHOT, q_novel: int = Q_NOVEL, q_base_total: int = Q_BASE_TOTAL):\n",
        "        # class-id -> list of local indices (within each Subset) for base/novel test splits\n",
        "        self.base_cti = base_cti\n",
        "        self.novel_cti = novel_cti\n",
        "        self.offset_base = offset_base\n",
        "\n",
        "        # explicit class-id pools\n",
        "        self.base_classes  = list(base_cti.keys())\n",
        "        self.novel_classes = list(novel_cti.keys())\n",
        "\n",
        "        # episode config\n",
        "        self.n_tasks = n_tasks\n",
        "        self.rng = rng\n",
        "        self.n_way = n_way\n",
        "        self.k_shot = k_shot\n",
        "        self.q_novel = q_novel\n",
        "        self.q_base_total = q_base_total\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_tasks\n",
        "\n",
        "    def __iter__(self):\n",
        "        for _ in range(self.n_tasks):\n",
        "            # ---- NOVEL BLOCK: pick N_WAY true novel classes and sample K+Qn per class (no replacement) ----\n",
        "            chosen_novel = self.rng.choice(self.novel_classes, size=self.n_way, replace=False)\n",
        "\n",
        "            novel_chunks = []\n",
        "            per_novel = self.k_shot + self.q_novel\n",
        "\n",
        "            for c in chosen_novel:\n",
        "                pool = self.novel_cti[c]                 # local indices within test_novel\n",
        "                if len(pool) < per_novel:\n",
        "                    raise ValueError(f\"Novel class {c} has {len(pool)} < {per_novel}\")\n",
        "                # sample K+Qn *without* replacement to avoid reusing the same image as support/query\n",
        "                idx = self.rng.choice(pool, size=per_novel, replace=False)\n",
        "                novel_chunks.append(idx)\n",
        "\n",
        "            # Flatten the novel part; indices are still in the \"novel namespace\" (no offset)\n",
        "            novel_block = np.concatenate(novel_chunks).astype(int)\n",
        "\n",
        "            # ---- BASE BLOCK: sample Qb indices from base part (optionally allow replacement) ----\n",
        "            base_q = []\n",
        "            while len(base_q) < self.q_base_total:\n",
        "                c = int(self.rng.choice(self.base_classes))\n",
        "                # sample a local index within test_base\n",
        "                cand_local = int(self.rng.choice(self.base_cti[c]))\n",
        "                # shift to address the second component of ConcatDataset\n",
        "                base_q.append(cand_local + self.offset_base)\n",
        "\n",
        "            base_block = np.array(base_q, dtype=int)\n",
        "\n",
        "            # ---- FINAL EPISODE ----\n",
        "            # Concatenate [novel | base] to match the downstream collate expectations\n",
        "            full_episode = np.concatenate([novel_block, base_block])\n",
        "            yield full_episode\n",
        "\n",
        "sampler_test = GFSLEvalEpisodeSampler(\n",
        "    cti_test_base, cti_test_novel, len(test_novel), n_tasks=TEST_TASKS, rng=test_rng,\n",
        "    n_way=N_WAY, k_shot=K_SHOT, q_novel=Q_NOVEL, q_base_total=Q_BASE_TOTAL,\n",
        ")"
      ],
      "metadata": {
        "id": "V1IuPKZRVVzL"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_collate(batch, n_way: int, k_shot: int, q_novel: int, q_base_total: int):\n",
        "    \"\"\"Reconstruct support/query tensors for a GFSL test episode.\n",
        "\n",
        "    Input `batch` is a list of (image, label) from `test_concat` where the first\n",
        "    N_WAY*(K_SHOT+Q_NOVEL) items belong to the novel subset and the remaining Q_BASE_TOTAL\n",
        "    items belong to the base subset.\n",
        "    \"\"\"\n",
        "    imgs, labs = list(zip(*batch))\n",
        "    images = torch.stack(imgs)\n",
        "    labels = torch.tensor([int(y) for y in labs])\n",
        "\n",
        "    per_novel = k_shot + q_novel\n",
        "    total_novel = n_way * per_novel\n",
        "\n",
        "    # reshape novel block into (N, K+Qn, C, H, W) and (N, K+Qn) for labels\n",
        "    novel_block = images[:total_novel].view(n_way, per_novel, *images.shape[1:])\n",
        "    novel_labels_block = labels[:total_novel].view(n_way, per_novel)\n",
        "\n",
        "    # split into support (first K) and query (last Qn)\n",
        "    support_novel = novel_block[:, :k_shot].reshape(-1, *images.shape[1:])\n",
        "    query_novel   = novel_block[:, k_shot:].reshape(-1, *images.shape[1:])\n",
        "    query_base    = images[total_novel:]  # remaining Qb from base subset\n",
        "\n",
        "    # concatenate all queries [novel | base]\n",
        "    query_images = torch.cat([query_novel, query_base], dim=0)\n",
        "\n",
        "    # collect true novel CIFAR IDs (one per class) and per-query GT labels\n",
        "    true_novel_ids = [int(novel_labels_block[i, 0].item()) for i in range(n_way)]\n",
        "    gt_novel = novel_labels_block[:, k_shot:].reshape(-1)\n",
        "    gt_base  = labels[total_novel:]\n",
        "\n",
        "    return support_novel, query_images, true_novel_ids, gt_novel, gt_base\n",
        "\n",
        "test_collate_fn = partial(\n",
        "    test_collate,\n",
        "    n_way=N_WAY, k_shot=K_SHOT, q_novel=Q_NOVEL, q_base_total=Q_BASE_TOTAL,\n",
        ")"
      ],
      "metadata": {
        "id": "PmIR_s8bVaDf"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a single test dataset by concatenating the two subsets.\n",
        "# Order matters: novel part first, base part second.\n",
        "test_concat = ConcatDataset([test_novel, test_base])"
      ],
      "metadata": {
        "id": "R0ZCnKHVfmo2"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader = DataLoader(\n",
        "    test_concat,\n",
        "    batch_sampler=sampler_test,   # emits indices into the single concatenated dataset\n",
        "    collate_fn=test_collate_fn,   # reconstructs (support/query etc.) from that batch\n",
        "    num_workers=2,\n",
        "    pin_memory=True,\n",
        ")"
      ],
      "metadata": {
        "id": "l5nkZ9_uvDbw"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.1.2 DFSLwF module**\n",
        "\n",
        "???"
      ],
      "metadata": {
        "id": "Q4Zt-0FOrjzs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvBlock(nn.Module):\n",
        "    \"\"\"Conv3x3 -> BN -> ReLU -> (optional MaxPool2d).\"\"\"\n",
        "    def __init__(self, in_ch, out_ch, pool: bool):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn   = nn.BatchNorm2d(out_ch)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.pool = nn.MaxPool2d(2) if pool else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x); x = self.bn(x); x = self.relu(x); x = self.pool(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class FeatureExtractor(nn.Module):\n",
        "    \"\"\"Feature extractor with selectable backbone: 'conv' (light) or 'resnet18'.\n",
        "\n",
        "    Args:\n",
        "        backbone: 'conv' or 'resnet18'.\n",
        "        normalize_out: if True, L2-normalize the output features.\n",
        "        resnet_pretrained: if True (only for 'resnet18'), load ImageNet pretrained weights.\n",
        "        remove_last_relu: if True (only for 'resnet18'), remove the last post-add ReLU in the final BasicBlock\n",
        "                          (useful with cosine classifiers, per DFSLwF ablations).\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        backbone: str = \"conv\",\n",
        "        normalize_out: bool = True,\n",
        "        resnet_pretrained: bool = True,\n",
        "        remove_last_relu: bool = True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.normalize_out = normalize_out\n",
        "\n",
        "        if backbone.lower() == \"conv\":\n",
        "            # Lightweight CIFAR-style CNN trained from scratch: 64-D output\n",
        "            self.fe = nn.Sequential(\n",
        "                ConvBlock(3,   64, pool=True),   # 32 -> 16\n",
        "                ConvBlock(64,  64, pool=True),   # 16 -> 8\n",
        "                ConvBlock(64,  64, pool=False),\n",
        "                ConvBlock(64,  64, pool=False),\n",
        "                nn.AdaptiveAvgPool2d(1),         # -> (B,64,1,1)\n",
        "            )\n",
        "            self._mode = \"conv\"\n",
        "            self.out_dim = 64\n",
        "\n",
        "        elif backbone.lower() == \"resnet18\":\n",
        "            weights = ResNet18_Weights.IMAGENET1K_V1 if resnet_pretrained else None\n",
        "            m = resnet18(weights=weights)\n",
        "            m.fc = nn.Identity()  # we want the 512-D penultimate features\n",
        "\n",
        "            if remove_last_relu:\n",
        "                # Patch only the final BasicBlock to skip the post-add ReLU\n",
        "                last_block = m.layer4[-1]\n",
        "\n",
        "                def forward_norelu(self_block, x):\n",
        "                    identity = x\n",
        "                    out = self_block.conv1(x); out = self_block.bn1(out); out = self_block.relu(out)\n",
        "                    out = self_block.conv2(out); out = self_block.bn2(out)\n",
        "                    if self_block.downsample is not None:\n",
        "                        identity = self_block.downsample(x)\n",
        "                    out = out + identity\n",
        "                    return out  # no final ReLU\n",
        "\n",
        "                last_block.forward = types.MethodType(forward_norelu, last_block)\n",
        "\n",
        "            self.fe = m\n",
        "            self._mode = \"resnet18\"\n",
        "            self.out_dim = 512\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown backbone '{backbone}'. Use 'conv' or 'resnet18'.\")\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if self._mode == \"conv\":\n",
        "            z = self.fe(x).squeeze(-1).squeeze(-1)    # (B, 64)\n",
        "        else:  # resnet18\n",
        "            z = self.fe(x)                            # (B, 512)\n",
        "        return l2_normalize(z, dim=1) if self.normalize_out else z"
      ],
      "metadata": {
        "id": "BmbYd7mBr2y3"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CosineClassifier(nn.Module):\n",
        "    \"\"\"Cosine classifier with learnable temperature τ (tau).\"\"\"\n",
        "    def __init__(self, in_dim: int, n_classes: int, init_scale: float = TAU_INIT):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.empty(n_classes, in_dim))\n",
        "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
        "        self.tau = nn.Parameter(torch.tensor(float(init_scale)))\n",
        "\n",
        "    def forward(self, feats: torch.Tensor) -> torch.Tensor:\n",
        "        W = l2_normalize(self.weight, dim=1)\n",
        "        feats = l2_normalize(feats, dim=1)\n",
        "        logits = feats @ W.t()\n",
        "        return self.tau * logits"
      ],
      "metadata": {
        "id": "6DmrwTPYr3Xd"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FewShotWeightGenerator(nn.Module):\n",
        "    \"\"\"\n",
        "    [DFSLwF] Few-shot classification weight generator = Avg + Attention:\n",
        "      - w_avg  = mean(z_i)                     → φ_avg ⊙ w_avg\n",
        "      - w_att  = avg_i softmax(γ cos(φ_q z_i, k_b)) · w_b (over base classes b) → φ_att ⊙ w_att\n",
        "      - w'     = φ_avg ⊙ w_avg + φ_att ⊙ w_att, then L2-normalize\n",
        "    Includes:\n",
        "      - learnable keys k_b (one per base class), size (C_base, D)\n",
        "      - learnable φ_q (Linear D→D, no bias), φ_avg, φ_att (vectors), γ (scalar)\n",
        "      - optional dropout on features during training (Stage-2)\n",
        "    \"\"\"\n",
        "    def __init__(self, dim: int, num_base: int, p_dropout: float = 0.5):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.num_base = num_base\n",
        "        self.phi_q = nn.Linear(dim, dim, bias=False)\n",
        "        nn.init.kaiming_uniform_(self.phi_q.weight, a=math.sqrt(5))\n",
        "        self.keys = nn.Parameter(l2_normalize(torch.randn(num_base, dim), dim=1))\n",
        "        self.phi_avg = nn.Parameter(torch.ones(dim))\n",
        "        self.phi_att = nn.Parameter(torch.ones(dim))\n",
        "        self.gamma = nn.Parameter(torch.tensor(10.0))  # attention temperature (like τ)\n",
        "        self.dropout = nn.Dropout(p=p_dropout)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        support_feats: torch.Tensor,          # (N*K, D), L2-normalized\n",
        "        base_weights: torch.Tensor,           # (C_base, D), not necessarily normalized\n",
        "        shots_per_class: int,\n",
        "        exclude_mask: Optional[torch.Tensor] = None  # (C_base,) bool; True = keep; False = exclude\n",
        "    ) -> torch.Tensor:\n",
        "        D = support_feats.size(1)\n",
        "        N = support_feats.size(0) // shots_per_class\n",
        "        z = support_feats.view(N, shots_per_class, D)\n",
        "        if self.training:\n",
        "            z = self.dropout(z)\n",
        "        # w_avg\n",
        "        w_avg = l2_normalize(z.mean(dim=1), dim=1)  # (N, D)\n",
        "        # attention over base weights\n",
        "        Wb = l2_normalize(base_weights, dim=1)      # (C_base, D)\n",
        "        Kb = l2_normalize(self.keys, dim=1)         # (C_base, D)\n",
        "        # queries\n",
        "        q = self.phi_q(z)                           # (N, K, D)\n",
        "        q = l2_normalize(q, dim=2)                 # normalize across D\n",
        "        # cosine(q, Kb) => (N, K, C_base)\n",
        "        att_logits = torch.einsum(\"nkd,bd->nkb\", q, Kb) * self.gamma\n",
        "        if exclude_mask is not None:\n",
        "            # set -inf on excluded classes before softmax\n",
        "            mask = exclude_mask.view(1, 1, -1)  # broadcast\n",
        "            att_logits = att_logits.masked_fill(~mask, float(\"-inf\"))\n",
        "        att = torch.softmax(att_logits, dim=2)      # (N, K, C_base)\n",
        "        # weighted sum of base weights -> (N, K, D), then average over K (shots)\n",
        "        w_att = torch.einsum(\"nkb,bd->nkd\", att, Wb).mean(dim=1)  # (N, D)\n",
        "        # combine\n",
        "        w = self.phi_avg * w_avg + self.phi_att * w_att            # Hadamard\n",
        "        w = l2_normalize(w, dim=1)  # (N, D)\n",
        "        return w"
      ],
      "metadata": {
        "id": "1SVW654R6Fmp"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DFSLwF(nn.Module):\n",
        "    def __init__(self, fe: FeatureExtractor, clf_base: CosineClassifier, gen: FewShotWeightGenerator):\n",
        "        super().__init__()\n",
        "        self.fe = fe\n",
        "        self.clf_base = clf_base\n",
        "        self.gen = gen\n",
        "\n",
        "    def forward_logits(self, x: torch.Tensor, novel_weights: torch.Tensor | None = None) -> torch.Tensor:\n",
        "        feats = self.fe(x)\n",
        "        Wb = l2_normalize(self.clf_base.weight, dim=1)\n",
        "        logits = self.clf_base.tau * (feats @ Wb.t())\n",
        "        if novel_weights is not None and novel_weights.numel() > 0:\n",
        "            Wn = l2_normalize(novel_weights, dim=1)\n",
        "            logits_n = self.clf_base.tau * (feats @ Wn.t())\n",
        "            logits = torch.cat([logits, logits_n], dim=1)\n",
        "        return logits\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def build_novel_weights(self, support_imgs: torch.Tensor, k_shot: int) -> torch.Tensor:\n",
        "        supp = self.fe(support_imgs)  # (N*K, D), already normalized\n",
        "        Wb = self.clf_base.weight\n",
        "        return self.gen(supp, Wb, k_shot, exclude_mask=None)"
      ],
      "metadata": {
        "id": "Sf1nyTmgsJmb"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fe  = FeatureExtractor(backbone=\"resnet18\", normalize_out=True, resnet_pretrained=True, remove_last_relu=True)\n",
        "fe  = FeatureExtractor(backbone=\"conv\", normalize_out=True)\n",
        "\n",
        "clf = CosineClassifier(in_dim=fe.out_dim, n_classes=len(base_order))\n",
        "gen = FewShotWeightGenerator(dim=fe.out_dim, num_base=len(base_order), p_dropout=0.5)\n",
        "\n",
        "model = DFSLwF(fe=fe, clf_base=clf, gen=gen).to(device)"
      ],
      "metadata": {
        "id": "zXx49_x5vS-U"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_model = copy.deepcopy(model)"
      ],
      "metadata": {
        "id": "V6-K32td3uTN"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.2 Training**\n",
        "\n",
        "???"
      ],
      "metadata": {
        "id": "pwqrdqc9DitQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.2.1 Stage 1: supervised base training**\n",
        "\n",
        "???"
      ],
      "metadata": {
        "id": "9rRpFEbisZky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_stage1(model: DFSLwF, loader: DataLoader, device: torch.device,\n",
        "                 epochs: int = STAGE1_EPOCHS, lr: float = STAGE1_LR,\n",
        "                 weight_decay: float = STAGE1_WEIGHT_DECAY,\n",
        "                 label_smoothing: float = STAGE1_LABEL_SMOOTH):\n",
        "\n",
        "    # [DFSLwF] Train feature extractor + base classifier (cosine)\n",
        "    model.fe.train(); model.clf_base.train(); model.gen.eval()\n",
        "\n",
        "    # Keep BN learnable here (paper trains a standard classifier in Stage-1)\n",
        "    params = list(model.fe.parameters()) + list(model.clf_base.parameters())\n",
        "    opt = torch.optim.Adam(params, lr=lr, weight_decay=weight_decay)\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
        "\n",
        "    pbar = tqdm(range(epochs), desc=\"[Stage1] Supervised base training\")\n",
        "    for _ in pbar:\n",
        "        losses = []\n",
        "        for xb, yb in loader:\n",
        "            xb = xb.to(device); yb = yb.to(device)\n",
        "            opt.zero_grad()\n",
        "            feats = model.fe(xb)\n",
        "            logits = model.clf_base(feats)\n",
        "            loss = criterion(logits, yb)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            losses.append(float(loss.item()))\n",
        "        pbar.set_postfix(loss=f\"{sliding_avg(losses, 50):.4f}\")"
      ],
      "metadata": {
        "id": "UKTZWOz4V6wh"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_stage1(model, train_loader_s1, device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boHxe6JoviaS",
        "outputId": "4153d508-296d-400b-9363-d7652de8835c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage1] Supervised base training: 100%|██████████| 120/120 [16:04<00:00,  8.03s/it, loss=1.5801]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stage1_model = copy.deepcopy(model)"
      ],
      "metadata": {
        "id": "MTUp7LJb31fW"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.2.2 Stage 2: episodic training**\n",
        "\n",
        "???"
      ],
      "metadata": {
        "id": "zYaBPBI4ssZq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_stage2(model: DFSLwF, meta_loader: DataLoader, device: torch.device,\n",
        "                 base_order: List[int], n_tasks: int = STAGE2_TASKS, val_every: int = STAGE2_VAL_EVERY,\n",
        "                 lr: float = STAGE2_LR):\n",
        "    \"\"\"\n",
        "    [DFSLwF] Freeze F, train generator + continue training W_base (and τ). Exclude pseudo-novel from attention memory.\n",
        "    Use *true* base labels for base queries; novel queries target indices are (Cb .. Cb+N-1).\n",
        "    \"\"\"\n",
        "    # Freeze feature extractor; freeze BN stats\n",
        "    model.fe.eval(); model.fe.apply(set_bn_eval)\n",
        "    for p in model.fe.parameters(): p.requires_grad = False\n",
        "\n",
        "    # Train generator, base weights, and tau\n",
        "    for p in model.clf_base.parameters(): p.requires_grad = True\n",
        "    params = list(model.gen.parameters()) + [model.clf_base.tau, model.clf_base.weight]\n",
        "    opt = torch.optim.Adam(params, lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # mapping CIFAR id -> local base index\n",
        "    b2local = {cid: i for i, cid in enumerate(sorted(base_order))}\n",
        "    Cb = model.clf_base.weight.size(0)\n",
        "\n",
        "    model.gen.train(); model.clf_base.train()\n",
        "    pbar = tqdm(enumerate(meta_loader), total=len(meta_loader), desc=\"[Stage2] Episodic Training\")\n",
        "    for step, (support_novel, query_images, true_novel_ids, base_q_labels_cifar) in pbar:\n",
        "        support_novel = support_novel.to(device)\n",
        "        query_images = query_images.to(device)\n",
        "        base_q_labels_cifar = base_q_labels_cifar.to(device)\n",
        "\n",
        "        # build exclude mask for attention memory (mask out pseudo-novel base classes)\n",
        "        exclude_mask = torch.ones(Cb, dtype=torch.bool, device=device)\n",
        "        for cid in true_novel_ids:\n",
        "            if cid in b2local:\n",
        "                exclude_mask[b2local[cid]] = False  # exclude these from memory\n",
        "\n",
        "        # Novel weights from supports (generator sees W_base and exclude mask)\n",
        "        with torch.no_grad():\n",
        "            supp_feats = model.fe(support_novel)  # (N*K, D)\n",
        "        novel_weights = model.gen(supp_feats, model.clf_base.weight, K_SHOT, exclude_mask=exclude_mask)  # (N, D)\n",
        "\n",
        "        # logits and targets\n",
        "        logits = model.forward_logits(query_images, novel_weights)  # layout [Cb | N]\n",
        "        N = novel_weights.size(0)\n",
        "\n",
        "        # targets: first N*Q_NOVEL are novel (indices Cb..Cb+N-1)\n",
        "        y_novel = torch.arange(N, device=device).repeat_interleave(Q_NOVEL)\n",
        "        targets = torch.empty(logits.size(0), dtype=torch.long, device=device)\n",
        "        targets[:N * Q_NOVEL] = Cb + y_novel\n",
        "\n",
        "        # base targets: map CIFAR ids -> local base indices\n",
        "        base_local = torch.tensor([b2local[int(y.item())] for y in base_q_labels_cifar], device=device)\n",
        "        targets[N * Q_NOVEL:] = base_local\n",
        "\n",
        "        loss = criterion(logits, targets)\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(params, STAGE2_GRAD_CLIP)\n",
        "        opt.step()\n",
        "        pbar.set_postfix(loss=f\"{float(loss.item()):.4f}\")"
      ],
      "metadata": {
        "id": "PmIOgTtNsmZF"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_stage2(model, train_loader_s2, device=device, base_order=base_order)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffRMFtefvnma",
        "outputId": "7f2e6c5a-cb21-4ebf-b4c1-54b2656e5c5d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Stage2] Meta (generator): 100%|██████████| 20000/20000 [20:18<00:00, 16.41it/s, loss=1.0335]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.3 Evaluation**\n",
        "\n",
        "???"
      ],
      "metadata": {
        "id": "DfsB1tdtsyKO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def evaluate_gfsl(model: DFSLwF, test_loader: DataLoader, cifar_targets_all: List[int],\n",
        "                  base_classes: List[int], device: torch.device) -> Tuple[float, float, float]:\n",
        "    model.fe.eval(); model.clf_base.eval(); model.gen.eval()\n",
        "    acc_per_episode_base, acc_per_episode_novel = [], []\n",
        "    base_order = sorted(base_classes)\n",
        "    b2local = {cid: i for i, cid in enumerate(base_order)}\n",
        "    Cb = model.clf_base.weight.size(0)\n",
        "\n",
        "    for (support_novel, query_images, true_novel_ids, gt_novel, gt_base) in test_loader:\n",
        "        support_novel = support_novel.to(device)\n",
        "        query_images = query_images.to(device)\n",
        "        gt_novel = gt_novel.to(device)\n",
        "        gt_base = gt_base.to(device)\n",
        "\n",
        "        novel_weights = model.build_novel_weights(support_novel, K_SHOT)  # (N, D)\n",
        "        logits = model.forward_logits(query_images, novel_weights)\n",
        "        preds = logits.argmax(dim=1)\n",
        "\n",
        "        N = novel_weights.size(0)\n",
        "        pred_novel = preds[:N * Q_NOVEL] - Cb\n",
        "        pred_base = preds[N * Q_NOVEL:]\n",
        "\n",
        "        id2local = {cid: i for i, cid in enumerate(true_novel_ids)}\n",
        "        gt_novel_local = torch.tensor([id2local[int(y.item())] for y in gt_novel], device=device)\n",
        "        gt_base_local = torch.tensor([b2local[int(y.item())] for y in gt_base], device=device)\n",
        "\n",
        "        acc_b = (pred_base == gt_base_local).float().mean().item()\n",
        "        acc_n = (pred_novel == gt_novel_local).float().mean().item()\n",
        "        acc_per_episode_base.append(acc_b); acc_per_episode_novel.append(acc_n)\n",
        "\n",
        "    return len(acc_per_episode_base), gfsl_stats(acc_per_episode_base, acc_per_episode_novel)"
      ],
      "metadata": {
        "id": "Q0WCFplb3CNH"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "T, stats = evaluate_gfsl(baseline_model, test_loader, ds_test.targets, base, device=device)\n",
        "print_stats(T, stats, model=\"Baseline model\")"
      ],
      "metadata": {
        "id": "Q_8s7dAK3-WC",
        "outputId": "84794d5e-3ae6-4a80-fedf-990c5b2490a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[test] - Baseline model (95% CI on 1000 tasks)\n",
            " - [Base]   acc=0.00% ± 0.00%\n",
            " - [Novel]  acc=38.73% ± 0.50%\n",
            " - [H-mean] acc=0.00% ± 0.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "T, stats = evaluate_gfsl(stage1_model, test_loader, ds_test.targets, base, device=device)\n",
        "print_stats(T, stats, model=\"After stage 1 training\")"
      ],
      "metadata": {
        "id": "GwAvzS9w3-Ja",
        "outputId": "7d7cc21b-2144-427d-ad18-b43225938d46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[test] - After stage 1 training (95% CI on 1000 tasks)\n",
            " - [Base]   acc=26.24% ± 0.32%\n",
            " - [Novel]  acc=64.12% ± 0.58%\n",
            " - [H-mean] acc=36.72% ± 0.33%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "T, stats = evaluate_gfsl(model, test_loader, ds_test.targets, base, device=device)\n",
        "print_stats(T, stats, model=\"Afetr stage 2 training\")"
      ],
      "metadata": {
        "id": "71N0971Mvqm6",
        "outputId": "5732007a-003e-4df1-ba80-b0e89552cfef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[test] - Afetr stage 2 training (95% CI on 1000 tasks)\n",
            " - [Base]   acc=34.83% ± 0.37%\n",
            " - [Novel]  acc=65.62% ± 0.59%\n",
            " - [H-mean] acc=44.93% ± 0.34%\n"
          ]
        }
      ]
    }
  ]
}